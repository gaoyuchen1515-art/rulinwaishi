{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaoyuchen1515-art/rulinwaishi/blob/main/CBS5502_Tutorial_pos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sve2XAV8pT5"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# üß† **CBS5502 ‚Äî Computational Linguistics and NLP Technologies**\n",
        "\n",
        "### üêç **2nd Python Tutorial**\n",
        "### üìÖ *February 4, 2026*\n",
        "\n",
        "---\n",
        "\n",
        "## üá®üá≥ **PoS Tagging and Disambiguation**\n",
        "---\n",
        "### üë®‚Äçüè´ **Instructor**\n",
        "**Dr. WAN Mingyu**\n",
        "\n",
        "### üë®‚Äçüè´ **Teaching Assistant**\n",
        "**Mr. BAO Xiaoyi**\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "## üåü Welcome!\n",
        "\n",
        "Welcome to the tutorial series of **CBS5502**!  \n",
        "In this tutorial, we will explore how **Part-of-Speech (PoS) tagging** works and how ambiguity can be resolved using **three different approaches**, all demonstrated with the classic ambiguous sentence:\n",
        "\n",
        "> **‚ÄúWe can can the can.‚Äù** üåü\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "- Understand what **PoS tagging** is and why it is important in NLP  \n",
        "- Identify **lexical and structural ambiguity** in natural language  \n",
        "- Apply **three approaches to PoS tagging**:\n",
        "  - Rule-based tagging\n",
        "  - Statistical / probabilistic tagging\n",
        "  - Dictionary‚Äë or library‚Äëbased tagging using Python  \n",
        "- Analyze and interpret tagging results for ambiguous sentences  \n",
        "\n",
        "---\n",
        "\n",
        "üöÄ Let‚Äôs Get Started!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.tag import hmm\n",
        "from collections import defaultdict\n",
        "from nltk.tag import brill, brill_trainer\n",
        "from nltk.tag import UnigramTagger, BigramTagger, DefaultTagger\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Ensure you have the required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"We can can the can.\"\n",
        "\n",
        "# Tokenizing the sentence into words\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(\"Tokenized Sentence:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "KKMAAcHjeUey",
        "outputId": "94592ed3-66fa-4264-fb1c-e1392872f644"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1155771201.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import required libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt_tab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2479\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2480\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1Ô∏è‚É£ Rule‚ÄëBased Approach\n",
        "\n",
        "#### üîç Overview\n",
        "The **rule‚Äëbased approach** assigns Part‚Äëof‚ÄëSpeech (PoS) tags using **handcrafted linguistic rules**, typically based on word forms, surrounding context, or fixed patterns.  \n",
        "This method does **not rely on training data**, making it easy to understand and implement.\n",
        "\n",
        "#### üß† How It Works\n",
        "- Each rule matches a word (or pattern) in the sentence\n",
        "- The **first matching rule** determines the PoS tag\n",
        "- Rules are applied **sequentially**, from top to bottom\n",
        "\n",
        "#### üß© Example Rules\n",
        "For our ambiguous sentence, we define a few **simple and intuitive rules**:\n",
        "- Tag **‚ÄúWe‚Äù** as a personal pronoun\n",
        "- Tag **‚Äúthe‚Äù** as a determiner\n",
        "- Assign **‚Äúcan‚Äù** a default modal‚Äëverb tag\n",
        "- Use a fallback rule for unknown cases\n",
        "\n",
        "These rules illustrate both the **strength** (clarity) and **limitation** (lack of context awareness) of the rule‚Äëbased approach."
      ],
      "metadata": {
        "id": "6dzn8wQkeyDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# STEP 1: Define default (most likely) POS tags\n",
        "# --------------------------------------------------\n",
        "# This dictionary provides a fallback tag for each word.\n",
        "# If no contextual rule applies, we use these tags.\n",
        "most_likely_tags = {\n",
        "    \"We\": \"PRP\",   # Personal pronoun\n",
        "    \"can\": \"MD\",   # Modal verb (default assumption)\n",
        "    \"the\": \"DT\"    # Determiner\n",
        "}"
      ],
      "metadata": {
        "id": "0YwOzmcwi_Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# STEP 2: Define the rule-based POS tagging function\n",
        "# --------------------------------------------------\n",
        "def rule_based_pos_tagger(tokens):\n",
        "    \"\"\"\n",
        "    Assign POS tags to a list of tokens using\n",
        "    handcrafted contextual rules.\n",
        "\n",
        "    Parameters:\n",
        "        tokens (list): A list of word tokens\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (word, POS tag) tuples\n",
        "    \"\"\"\n",
        "\n",
        "    tagged_sentence = []  # Store the final tagged output\n",
        "\n",
        "    # Iterate through each word with its position\n",
        "    for i, word in enumerate(tokens):\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # STEP 3: Apply context-sensitive rules\n",
        "        # --------------------------------------------------\n",
        "\n",
        "        # Rule 1:\n",
        "        # If \"can\" appears immediately after \"We\",\n",
        "        # it functions as a modal verb (e.g., \"We can ...\")\n",
        "        if word == \"can\" and i > 0 and tokens[i - 1] == \"We\":\n",
        "            tag = \"MD\"\n",
        "\n",
        "        # Rule 2:\n",
        "        # If \"can\" follows \"the\", it is treated as a noun\n",
        "        # (e.g., \"the can\")\n",
        "        elif word == \"can\" and i > 0 and tokens[i - 1] == \"the\":\n",
        "            tag = \"NN\"\n",
        "\n",
        "        # Rule 3:\n",
        "        # If \"can\" follows another \"can\",\n",
        "        # it is treated as a main verb\n",
        "        # (e.g., \"can can the...\")\n",
        "        elif word == \"can\" and i > 0 and tokens[i - 1] == \"can\":\n",
        "            tag = \"VB\"\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # STEP 4: Apply default rule\n",
        "        # --------------------------------------------------\n",
        "        # If no specific contextual rule matches,\n",
        "        # fall back to the most likely tag\n",
        "        else:\n",
        "            tag = most_likely_tags.get(word, \"NN\")\n",
        "            # Unknown words default to NN (noun)\n",
        "\n",
        "        # Add the (word, tag) pair to the result\n",
        "        tagged_sentence.append((word, tag))\n",
        "\n",
        "    return tagged_sentence"
      ],
      "metadata": {
        "id": "8xlieTqzjD2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîé Step‚Äëby‚ÄëStep Rule Application\n",
        "\n",
        "Sentence: **We can can the can .**\n",
        "\n",
        "| Position | Token | Left Context | Applied Rule | Assigned Tag |\n",
        "|---------:|-------|--------------|--------------|--------------|\n",
        "| 0 | We | ‚Äî | Default dictionary rule | PRP |\n",
        "| 1 | can | We | Rule 1: *can* after *We* | MD |\n",
        "| 2 | can | can | Rule 3: *can* after *can* | VB |\n",
        "| 3 | the | can | Default dictionary rule | DT |\n",
        "| 4 | can | the | Rule 2: *can* after *the* | NN |\n",
        "| 5 | . | can | Default fallback | NN |"
      ],
      "metadata": {
        "id": "r9blpkyokByy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# STEP 5: Apply the rule-based tagger\n",
        "# --------------------------------------------------\n",
        "rule_based_tags = rule_based_pos_tagger(tokens)\n",
        "\n",
        "# Display the result\n",
        "print(\"Rule-Based POS Tags:\")\n",
        "for word, tag in rule_based_tags:\n",
        "    print(f\"{word:>5}  ‚Üí  {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Na_Cu88ijcEi",
        "outputId": "915558b9-7342-46e2-a6d8-baea197acc1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rule_based_pos_tagger' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1517147727.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# STEP 5: Apply the rule-based tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# --------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrule_based_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule_based_pos_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rule_based_pos_tagger' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö†Ô∏è Error Cases & Discussion\n",
        "\n",
        "### Example 1\n",
        "Sentence: **They can fish.**\n",
        "\n",
        "Expected:\n",
        "- can ‚Üí MD\n",
        "- fish ‚Üí VB\n",
        "\n",
        "Rule-Based Output:\n",
        "- can ‚Üí MD ‚úÖ\n",
        "- fish ‚Üí NN ‚ùå\n",
        "\n",
        "üìå *Why?*  \n",
        "The system lacks a rule recognizing **verb usage without ‚Äúthe‚Äù**."
      ],
      "metadata": {
        "id": "2V9TJjNUkXlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2\n",
        "Sentence: **The can can rust.**\n",
        "\n",
        "Correct interpretation:\n",
        "- can ‚Üí NN\n",
        "- can ‚Üí VB\n",
        "\n",
        "Rule-Based Output:\n",
        "- can ‚Üí NN ‚úÖ\n",
        "- can ‚Üí VB ‚úÖ (by coincidence)\n",
        "\n",
        "üìå *Discussion point:*  \n",
        "Correct tagging here is **accidental**, not robust."
      ],
      "metadata": {
        "id": "yWPylpDfkeT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Teaching Notes\n",
        "\n",
        "- This approach relies entirely on **manually written rules**\n",
        "- Each rule encodes **explicit linguistic intuition**\n",
        "\n",
        "#### ‚úÖ Strengths\n",
        "- Easy to understand and interpret\n",
        "- Transparent decision‚Äëmaking process\n",
        "\n",
        "#### ‚ùå Limitations\n",
        "- Difficult to scale to large vocabularies\n",
        "- Brittle when encountering unseen or unexpected patterns\n",
        "\n",
        "### üß† Learning Takeaways\n",
        "\n",
        "- Rule-based tagging makes **linguistic assumptions explicit**\n",
        "- Context helps, but only when **manually encoded**\n",
        "- Error cases reveal why **learning from data is necessary**\n",
        "- HMM and Brill taggers automate what rules attempt to approximate"
      ],
      "metadata": {
        "id": "gqJ-_KYZjrv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Hidden Markov Model (HMM) Approach\n",
        "\n",
        "### üîç Overview\n",
        "The **Hidden Markov Model (HMM)** approach is a **statistical sequence‚Äëlabeling method** that assigns PoS tags by modeling language as a **probabilistic process**.  \n",
        "It predicts the **most likely sequence of tags** for a sentence using probabilities learned from a **tagged corpus**.\n",
        "\n",
        "### üß† Core Assumptions\n",
        "HMM PoS tagging relies on two key assumptions:\n",
        "\n",
        "- **Markov Assumption:**  \n",
        "  The current tag depends only on a limited number of previous tags (typically one or two).\n",
        "- **Output Independence Assumption:**  \n",
        "  Each word is generated independently given its tag.\n",
        "\n",
        "### üîÅ Decoding Strategy\n",
        "To determine the optimal tag sequence, HMMs use the **Viterbi algorithm**, which efficiently finds:\n",
        "\n",
        "> ‚úÖ The most probable tag sequence for the entire sentence,  \n",
        "> rather than tagging each word independently.\n",
        "\n"
      ],
      "metadata": {
        "id": "nUKdbj5elX10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Step 1: What Does an HMM Model?\n",
        "\n",
        "An HMM models language with two probability components:\n",
        "\n",
        "1. **Transition Probability**\n",
        "   - $$P(t_i \\mid t_{i-1})$$  \n",
        "   - How likely one tag follows another\n",
        "\n",
        "2. **Emission Probability**\n",
        "   - $$P(w_i \\mid t_i)$$  \n",
        "   - How likely a word is generated by a tag\n",
        "\n",
        "The goal is to find the **most probable tag sequence** for the entire sentence."
      ],
      "metadata": {
        "id": "qJ0KW74sl_Lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ Step 2: Why We Need Sequence‚ÄëLevel Decisions\n",
        "\n",
        "Ambiguous words like **‚Äúcan‚Äù** cannot be tagged reliably in isolation.\n",
        "\n",
        "‚úÖ HMMs solve this by:\n",
        "- Considering **previous tags**\n",
        "- Evaluating the **entire sentence**\n",
        "- Using **global optimization** via the Viterbi algorithm"
      ],
      "metadata": {
        "id": "j58EFuGPmcm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Step 3: Environment Setup"
      ],
      "metadata": {
        "id": "zIdsEKEBmgT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import hmm"
      ],
      "metadata": {
        "id": "bX7yBkUYmia-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "kwEZzDc8ml6W",
        "outputId": "2e0a107b-6d08-47a4-99c0-e2595c99cf6c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nltk' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-893122810.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download resources (run once)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'brown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìö Step 4: Prepare Training Data\n",
        "\n",
        "HMMs require a **tagged corpus** to learn probabilities.\n",
        "Here, we use the Brown Corpus (news category)."
      ],
      "metadata": {
        "id": "b-Kf0joDmpoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üè∑Ô∏è Official Tag List for `hmm_tagger` (Brown Tagset)\n",
        "\n",
        "### ‚úÖ Primary Reference (Recommended for Teaching)\n",
        "\n",
        "You can find the **complete Brown Corpus tagset**, along with detailed explanations, in the **NLTK Book, Chapter 5**:\n",
        "\n",
        "üëâ **NLTK Book ‚Äî Categorizing and Tagging Words (Brown Tagset)**  \n",
        "*(nltk.org)*\n",
        "\n",
        "### üìö What This Section Covers\n",
        "\n",
        "This reference documents:\n",
        "\n",
        "- ‚úÖ All Brown tags (e.g. `PPSS`, `AT`, `NP`, `VB`, `MD`)\n",
        "- ‚úÖ Examples of words annotated with each tag\n",
        "- ‚úÖ Key differences between the **Brown tagset** and the **Penn Treebank tagset**\n",
        "\n",
        "üìå **Teaching note:**  \n",
        "The `HiddenMarkovModelTagger` in NLTK inherits its tagset directly from the corpus it is trained on. When trained with the Brown Corpus, it therefore produces **Brown-style PoS tags**."
      ],
      "metadata": {
        "id": "PG-zVJOKoNcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tagged sentences for training\n",
        "train_sentences = brown.tagged_sents(categories='news')\n",
        "\n",
        "# Inspect one example\n",
        "train_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "BF_4HDGcmuQg",
        "outputId": "f80c20ca-2516-492b-d981-895aa0a0d8bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'brown' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3262031457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load tagged sentences for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'news'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Inspect one example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'brown' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üèóÔ∏è Step 5: Train the HMM Tagger\n",
        "\n",
        "The training process automatically learns:\n",
        "- Tag transition probabilities\n",
        "- Word emission probabilities"
      ],
      "metadata": {
        "id": "wXlCY1mtm0An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an HMM tagger\n",
        "hmm_tagger = hmm.HiddenMarkovModelTagger.train(train_sentences)"
      ],
      "metadata": {
        "id": "yyHLA67Sm23X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "3f3f6051-83b5-4de2-813e-4a4c5934cd71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hmm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4212796608.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train an HMM tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhmm_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddenMarkovModelTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'hmm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Step 6: Apply the HMM to an Ambiguous Sentence"
      ],
      "metadata": {
        "id": "60KICVVfm6Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"We can can the can .\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "hmm_tags = hmm_tagger.tag(tokens)\n",
        "hmm_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Z0OC2snFm8H3",
        "outputId": "47fb75ca-201c-4f56-e0fb-ea9bbefa16cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nltk' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1906184146.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"We can can the can .\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhmm_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhmm_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úî The HMM correctly captures grammatical structure  \n",
        "‚úî Ambiguity is resolved using contextual probabilities  \n",
        "‚úî The same word can have different tags in the same sentence"
      ],
      "metadata": {
        "id": "J5fmq__PndPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Transformation‚ÄëBased (Brill) Tagging\n",
        "\n",
        "### üîç Overview\n",
        "The **Transformation‚ÄëBased Approach**, also known as **Brill Tagging**, is a **hybrid method** that combines:\n",
        "\n",
        "- ‚úÖ A **simple statistical baseline tagger**\n",
        "- ‚úÖ A set of **learned transformation rules**\n",
        "\n",
        "Instead of assigning tags in one step, Brill tagging **iteratively corrects errors** made by an initial tagger using rules learned from a **tagged corpus**.\n"
      ],
      "metadata": {
        "id": "7yMelAMli9kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Core Idea\n",
        "\n",
        "Brill tagging follows this learning cycle:\n",
        "\n",
        "1. Start with a **baseline tagger** (e.g., Unigram Tagger)\n",
        "2. Compare its output with **gold‚Äëstandard tags**\n",
        "3. Learn **transformation rules** that reduce errors\n",
        "4. Apply the rules sequentially to improve tagging accuracy\n",
        "\n",
        "üìå The learned rules are **human‚Äëreadable**, making this approach both **accurate and interpretable**."
      ],
      "metadata": {
        "id": "Ga95uh8Io8pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Step 1: Environment Setup"
      ],
      "metadata": {
        "id": "6ay9cDUOpEsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sklearn-crfsuite"
      ],
      "metadata": {
        "id": "zps22o2LpGV1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = brown.tagged_sents(categories='news')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "w_N-oahorEnl",
        "outputId": "b9d69e3d-4e81-4598-e3a3-efe2682bee99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1481151338.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'brown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2479\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2480\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import DefaultTagger, UnigramTagger\n",
        "\n",
        "default_tagger = DefaultTagger('NN')\n",
        "baseline_tagger = UnigramTagger(sentences, backoff=default_tagger)"
      ],
      "metadata": {
        "id": "uG6RK34hrIUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"We can can the can .\".split()\n",
        "baseline_tagger.tag(sentence)"
      ],
      "metadata": {
        "id": "kolHV1FBrMG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word.lower()': word.lower(),\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        features['prev_word'] = sent[i-1]\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        features['next_word'] = sent[i+1]\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "yt52IRuNrPW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2features(sent):\n",
        "    return [word_features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [tag for _, tag in sent]\n",
        "\n",
        "X = [sent2features([w for w, t in s]) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]"
      ],
      "metadata": {
        "id": "bNhXzEB1rTCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_subset = sentences[:500]\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    max_iterations=10,\n",
        "    all_possible_transitions=False\n",
        ")\n",
        "\n",
        "crf.fit(X_small[:500], y_small[:500])"
      ],
      "metadata": {
        "id": "zAEVZpZIrW2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"We can can the can .\".split()\n",
        "features = sent2features(test_sentence)\n",
        "\n",
        "list(zip(test_sentence, crf.predict_single(features)))"
      ],
      "metadata": {
        "id": "rdzWfDsgrdDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö†Ô∏è Practical Note on CRF Training\n",
        "\n",
        "CRFs are powerful but computationally expensive models.\n",
        "Training on large corpora with all possible tag transitions\n",
        "can take a very long time.\n",
        "\n",
        "‚úÖ For teaching and experimentation:\n",
        "- Use a small subset of data\n",
        "- Limit the number of iterations\n",
        "- Disable unnecessary transitions\n",
        "\n",
        "This preserves the learning behavior while keeping runtime manageable."
      ],
      "metadata": {
        "id": "FmQnDz9qwm4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üß† Interpretation\n",
        "\n",
        "- **Rule‚ÄëBased Tagger**\n",
        "  - Correctly handles this sentence due to carefully designed rules.\n",
        "  - Performance is **fragile** and depends entirely on manual rule coverage.\n",
        "\n",
        "- **Hidden Markov Model (HMM)**\n",
        "  - Resolves ambiguity using **learned transition and emission probabilities**.\n",
        "  - Makes **global sequence‚Äëlevel decisions**, leading to robust results.\n",
        "\n",
        "- **Brill‚ÄëStyle (Transformation‚ÄëBased) Tagger**\n",
        "  - Starts from a weak baseline and **learns contextual corrections**.\n",
        "  - Combines the **interpretability of rules** with **data‚Äëdriven learning**.\n",
        "  - Often outperforms unigram or bigram taggers when **training data is limited**.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Key Takeaway\n",
        "\n",
        "Although all three methods succeed on this example, they do so for different reasons:\n",
        "\n",
        "- Rule‚Äëbased tagging relies on **explicit linguistic intuition**\n",
        "- HMM tagging relies on **probabilistic sequence modeling**\n",
        "- Brill‚Äëstyle tagging bridges both worlds by **learning rules from data**\n",
        "\n",
        "This comparison highlights why transformation‚Äëbased methods remain an important conceptual bridge between symbolic and statistical NLP approaches."
      ],
      "metadata": {
        "id": "DnDNJW0QkdBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Playground ‚Äî Ending Exercises\n",
        "\n",
        "The following exercises encourage you to **apply, compare, and reflect** on the three PoS tagging approaches covered in this tutorial. Focus on **ambiguity**, **context**, and **model behavior** rather than just correctness.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Exercise 1: English Ambiguity Challenge  \n",
        "**Sentence:**  \n",
        "> *‚ÄúTime flies like an arrow.‚Äù*\n",
        "\n",
        "This sentence is famously ambiguous and can be interpreted in multiple ways.\n",
        "\n",
        "#### ‚úÖ Tasks\n",
        "1. **Tokenize** the sentence.\n",
        "2. Apply:\n",
        "   - Rule‚Äëbased tagging  \n",
        "   - HMM tagging  \n",
        "   - Brill‚Äëstyle (transformation‚Äëbased) tagging\n",
        "3. Record the PoS tags produced by each method.\n",
        "\n",
        "#### üß† Guiding Questions\n",
        "- Which word(s) show different PoS tags across methods?\n",
        "- Does *flies* behave as a **noun** or a **verb**?\n",
        "- Is *like* treated as a **verb**, **preposition**, or **conjunction**?\n",
        "- Which approach best captures the intended reading?\n",
        "\n",
        "#### üí° Reflection\n",
        "- Why is this sentence difficult to tag correctly without full syntactic analysis?\n",
        "- How does sequence‚Äëlevel modeling help resolve ambiguity?\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Exercise 2: Chinese Structural Ambiguity  \n",
        "**Sentence:**  \n",
        "> **ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫„ÄÇ**  \n",
        "> *(Pinyin: W«í x«êhuƒÅn chƒ´ p√≠nggu«í de r√©n.)*\n",
        "\n",
        "This sentence is a classic example used in **Chinese NLP** to test ambiguity resolution.\n",
        "\n",
        "#### ‚úÖ Tasks\n",
        "1. Segment the sentence into words (use a Chinese tokenizer if available).\n",
        "2. Assign PoS tags to each word.\n",
        "3. Identify at least **two possible interpretations** of the sentence.\n",
        "\n",
        "#### üß† Key Points to Consider\n",
        "- The grammatical role of **‚ÄúÁöÑ‚Äù**\n",
        "- Whether **‚ÄúÂêÉËãπÊûú‚Äù** modifies:\n",
        "  - *Êàë* (I like to eat apples), or\n",
        "  - *‰∫∫* (people who eat apples)\n",
        "- How relative clauses are formed in Chinese\n",
        "\n",
        "#### üí° Reflection\n",
        "- Why is **‚ÄúÁöÑ‚Äù** challenging for PoS tagging and parsing?\n",
        "- What additional information (syntax, semantics, or context) would help disambiguate the sentence?\n",
        "- Why do purely rule‚Äëbased approaches struggle with this example?\n",
        "\n",
        "---\n",
        "\n",
        "### üåü Take‚ÄëHome Insight\n",
        "\n",
        "These exercises illustrate that:\n",
        "\n",
        "- **PoS tagging alone is often insufficient** for full disambiguation\n",
        "- Ambiguity exists at both **lexical** and **structural** levels\n",
        "- Real‚Äëworld NLP systems must integrate **context, syntax, and semantics**\n",
        "\n",
        "‚úÖ Congratulations on completing the tutorial!"
      ],
      "metadata": {
        "id": "nNuN6ogTgsZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall nltk -y\n",
        "!pip uninstall sklearn-crfsuite -y\n",
        "!pip uninstall jieba -y\n",
        "!pip uninstall pkuseg -y\n",
        "\n",
        "# Ê∏ÖÁêÜ pip ÁºìÂ≠ò\n",
        "!pip cache purge\n",
        "\n",
        "# ÂÆâË£ÖÊâÄÊúâÈúÄË¶ÅÁöÑÂ∫ì\n",
        "!pip install nltk==3.8.1\n",
        "!pip install numpy\n",
        "!pip install jieba\n",
        "\n",
        "print(\"ÂÆâË£ÖÂÆåÊàêÔºÅÁé∞Âú®ÈáçÊñ∞ÂêØÂä®ËøêË°åÊó∂...\")"
      ],
      "metadata": {
        "id": "dPA_s1MxKiNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Exercise 1: English Ambiguity Analysis\n",
        "# ============================\n",
        "\n",
        "# Clean installation\n",
        "!pip uninstall nltk -y -q\n",
        "!pip install nltk==3.9.0 -q  # Use compatible version\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "    nltk.download('brown', quiet=True)\n",
        "except:\n",
        "    # Alternative download method\n",
        "    import ssl\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXERCISE 1: English Ambiguity - 'Time flies like an arrow'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define the sentence\n",
        "sentence = \"Time flies like an arrow\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(f\"Original sentence: {sentence}\")\n",
        "print(f\"Tokenized: {tokens}\\n\")\n",
        "\n",
        "# ========== Method 1: Rule-based Tagging ==========\n",
        "print(\"Method 1: Rule-based POS Tagging\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Define tagging rules\n",
        "rule_based_tags = []\n",
        "\n",
        "for i, word in enumerate(tokens):\n",
        "    word_lower = word.lower()\n",
        "\n",
        "    # Rule 1: First word \"Time\" is likely a noun\n",
        "    if i == 0 and word_lower == \"time\":\n",
        "        rule_based_tags.append((word, \"NN\"))  # Noun\n",
        "\n",
        "    # Rule 2: \"flies\" could be verb or noun\n",
        "    elif word_lower == \"flies\":\n",
        "        if i > 0 and tokens[i-1].lower() == \"time\":\n",
        "            rule_based_tags.append((word, \"VBZ\"))  # Verb 3rd person singular\n",
        "        else:\n",
        "            rule_based_tags.append((word, \"NNS\"))  # Noun plural\n",
        "\n",
        "    # Rule 3: \"like\" could be preposition or verb\n",
        "    elif word_lower == \"like\":\n",
        "        if i > 0 and tokens[i-1].lower() in [\"flies\", \"fly\"]:\n",
        "            rule_based_tags.append((word, \"IN\"))  # Preposition\n",
        "        else:\n",
        "            rule_based_tags.append((word, \"VB\"))  # Verb\n",
        "\n",
        "    # Rule 4: \"an\" is a determiner\n",
        "    elif word_lower == \"an\":\n",
        "        rule_based_tags.append((word, \"DT\"))\n",
        "\n",
        "    # Rule 5: \"arrow\" is a noun\n",
        "    elif word_lower == \"arrow\":\n",
        "        rule_based_tags.append((word, \"NN\"))\n",
        "\n",
        "    # Default rule\n",
        "    else:\n",
        "        rule_based_tags.append((word, \"UNK\"))\n",
        "\n",
        "print(\"Rule-based tagging results:\")\n",
        "for word, tag in rule_based_tags:\n",
        "    print(f\"  {word}: {tag}\")\n",
        "\n",
        "# ========== Method 2: NLTK Default Tagger ==========\n",
        "print(\"\\nMethod 2: NLTK Default Tagger (Transformation-based)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Use NLTK's pre-trained tagger\n",
        "default_tags = nltk.pos_tag(tokens)\n",
        "print(\"NLTK tagging results:\")\n",
        "for word, tag in default_tags:\n",
        "    print(f\"  {word}: {tag}\")\n",
        "\n",
        "# ========== Method 3: Create a Custom Tagger ==========\n",
        "print(\"\\nMethod 3: Custom Pattern-based Tagger\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Define patterns for Brill-style tagging\n",
        "patterns = [\n",
        "    (r'.*ing$', 'VBG'),               # gerunds\n",
        "    (r'.*ed$', 'VBD'),                # simple past\n",
        "    (r'.*es$', 'VBZ'),                # 3rd singular present\n",
        "    (r'.*ould$', 'MD'),               # modals\n",
        "    (r'.*\\'s$', 'POS'),               # possessive\n",
        "    (r'.*s$', 'NNS'),                 # plural nouns\n",
        "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
        "    (r'.*', 'NN')                     # default: noun\n",
        "]\n",
        "\n",
        "# Create a regex tagger\n",
        "from nltk import RegexpTagger\n",
        "regex_tagger = RegexpTagger(patterns)\n",
        "regex_tags = regex_tagger.tag(tokens)\n",
        "\n",
        "print(\"Pattern-based tagging results:\")\n",
        "for word, tag in regex_tags:\n",
        "    print(f\"  {word}: {tag}\")\n",
        "\n",
        "# ========== Analysis and Comparison ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ANALYSIS AND COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nTagging results comparison:\")\n",
        "print(f\"{'Word':<8} {'Rule-based':<12} {'NLTK':<12} {'Pattern-based':<12}\")\n",
        "print(\"-\" * 45)\n",
        "for i, word in enumerate(tokens):\n",
        "    rule_tag = rule_based_tags[i][1]\n",
        "    nltk_tag = default_tags[i][1]\n",
        "    pattern_tag = regex_tags[i][1]\n",
        "    print(f\"{word:<8} {rule_tag:<12} {nltk_tag:<12} {pattern_tag:<12}\")\n",
        "\n",
        "print(\"\\n1. Key ambiguity points:\")\n",
        "print(\"   - 'flies': Can be verb (VBZ) or noun (NNS)\")\n",
        "print(\"   - 'like': Can be preposition (IN) or verb (VB)\")\n",
        "print(\"   - 'Time': Always noun (NN) in this context\")\n",
        "\n",
        "print(\"\\n2. Two interpretations:\")\n",
        "print(\"   Interpretation A: 'Time passes quickly like an arrow'\")\n",
        "print(\"   - Structure: Time(NN) flies(VBZ) like(IN) an(DT) arrow(NN)\")\n",
        "print(\"   - Meaning: Time moves as fast as an arrow\")\n",
        "print(\"\\n   Interpretation B: 'Time-flies (insects) enjoy an arrow'\")\n",
        "print(\"   - Structure: Time(NN) flies(NNS) like(VB) an(DT) arrow(NN)\")\n",
        "print(\"   - Meaning: Insects called 'time flies' like arrows\")\n",
        "\n",
        "print(\"\\n3. Method evaluation:\")\n",
        "print(\"   - Rule-based: Good for specific cases, needs manual tuning\")\n",
        "print(\"   - NLTK: Most reliable, uses statistical and rule-based hybrid\")\n",
        "print(\"   - Pattern-based: Good fallback, uses word endings and patterns\")\n",
        "\n",
        "# ============================\n",
        "# Exercise 2: Chinese Sentence Analysis\n",
        "# ============================\n",
        "print(\"\\n\\n\" + \"=\" * 60)\n",
        "print(\"EXERCISE 2: Chinese Sentence Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"NOTE: Chinese analysis requires additional libraries.\")\n",
        "print(\"We'll analyze the sentence conceptually instead.\")\n",
        "\n",
        "# Chinese sentence information\n",
        "chinese_sentence = \"ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫\"\n",
        "print(f\"\\nChinese sentence: {chinese_sentence}\")\n",
        "print(f\"Pinyin: W«í x«êhuƒÅn chƒ´ p√≠nggu«í de r√©n\")\n",
        "print(f\"Literal: I like eat apple DE person\")\n",
        "print(f\"Common translation: 'I like people who eat apples' or 'I like to eat apples'\")\n",
        "\n",
        "# ========== Manual Analysis ==========\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"MANUAL LINGUISTIC ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(\"\\nWord-by-word breakdown:\")\n",
        "print(\"  Êàë (w«í) - I/me (pronoun)\")\n",
        "print(\"  ÂñúÊ¨¢ (x«êhuƒÅn) - to like (verb)\")\n",
        "print(\"  ÂêÉ (chƒ´) - to eat (verb)\")\n",
        "print(\"  ËãπÊûú (p√≠nggu«í) - apple (noun)\")\n",
        "print(\"  ÁöÑ (de) - possessive/relative particle\")\n",
        "print(\"  ‰∫∫ (r√©n) - person/people (noun)\")\n",
        "\n",
        "print(\"\\nTwo possible structures:\")\n",
        "print(\"\\n1. Structure A: Êàë + ÂñúÊ¨¢ + [ÂêÉËãπÊûú + ÁöÑ + ‰∫∫]\")\n",
        "print(\"   Parse: I + like + [people who eat apples]\")\n",
        "print(\"   Meaning: I like people who eat apples\")\n",
        "print(\"   POS: PRON + VERB + [VERB + NOUN + PART + NOUN]\")\n",
        "\n",
        "print(\"\\n2. Structure B: Êàë + ÂñúÊ¨¢ + ÂêÉ + ËãπÊûú\")\n",
        "print(\"   Parse: I + like + eat + apples\")\n",
        "print(\"   Meaning: I like to eat apples ('ÁöÑ‰∫∫' is redundant)\")\n",
        "print(\"   POS: PRON + VERB + VERB + NOUN\")\n",
        "\n",
        "print(\"\\n3. Structure C: [ÊàëÂñúÊ¨¢ÂêÉËãπÊûú] + ÁöÑ + ‰∫∫\")\n",
        "print(\"   Parse: [I like to eat apples] + 's + person\")\n",
        "print(\"   Meaning: The person who likes to eat apples\")\n",
        "print(\"   POS: [PRON + VERB + VERB + NOUN] + PART + NOUN\")\n",
        "\n",
        "print(\"\\nAmbiguity source:\")\n",
        "print(\"  - The particle 'ÁöÑ' can create relative clauses\")\n",
        "print(\"  - Chinese allows noun phrases without explicit relative pronouns\")\n",
        "print(\"  - Context determines the correct parse\")\n",
        "\n",
        "# ========== Comparison with English ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON: ENGLISH vs CHINESE AMBIGUITY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nEnglish 'Time flies like an arrow':\")\n",
        "print(\"  - Type: Lexical and structural ambiguity\")\n",
        "print(\"  - Resolution: POS tagging + syntax parsing\")\n",
        "print(\"  - Key words: 'flies'(V/N), 'like'(P/V)\")\n",
        "\n",
        "print(\"\\nChinese 'ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫':\")\n",
        "print(\"  - Type: Structural/syntactic ambiguity\")\n",
        "print(\"  - Resolution: Phrase structure parsing\")\n",
        "print(\"  - Key element: Particle 'ÁöÑ' (creates relative clauses)\")\n",
        "\n",
        "print(\"\\nSimilarities:\")\n",
        "print(\"  - Both require context for full disambiguation\")\n",
        "print(\"  - Both have multiple valid interpretations\")\n",
        "print(\"  - Syntax plays crucial role in meaning\")\n",
        "\n",
        "print(\"\\nDifferences:\")\n",
        "print(\"  - English: Word-level ambiguity (same word, different POS)\")\n",
        "print(\"  - Chinese: Phrase-level ambiguity (same sequence, different structures)\")\n",
        "print(\"  - English uses word order and function words\")\n",
        "print(\"  - Chinese uses particles and context\")\n",
        "\n",
        "# ========== Advanced Analysis ==========\n",
        "print(\"\\n\\n\" + \"=\" * 60)\n",
        "print(\"ADVANCED ANALYSIS: TUTORIAL EXAMPLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test the tutorial examples\n",
        "tutorial_sentences = [\n",
        "    \"We can can the can\",\n",
        "    \"Fruit flies like a banana\",\n",
        "    \"The old man the boat\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting additional ambiguous sentences:\")\n",
        "\n",
        "for i, sent in enumerate(tutorial_sentences, 1):\n",
        "    print(f\"\\n{i}. Sentence: '{sent}'\")\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    print(f\"   Tokens: {tokens}\")\n",
        "    print(f\"   POS tags: {tags}\")\n",
        "\n",
        "    # Specific analysis for each sentence\n",
        "    if i == 1:\n",
        "        print(\"   Analysis: 'can' appears 3 times with different POS:\")\n",
        "        print(\"     - Position 1: Modal verb (ability)\")\n",
        "        print(\"     - Position 2: Verb (to preserve)\")\n",
        "        print(\"     - Position 4: Noun (container)\")\n",
        "    elif i == 2:\n",
        "        print(\"   Analysis: Same ambiguity as 'Time flies like an arrow'\")\n",
        "        print(\"     - 'flies': Can be noun (insects) or verb\")\n",
        "        print(\"     - 'like': Can be preposition or verb\")\n",
        "    elif i == 3:\n",
        "        print(\"   Analysis: Garden path sentence\")\n",
        "        print(\"     - Initial parse: 'The old man' (noun phrase)\")\n",
        "        print(\"     - Correct parse: 'The old' (noun) + 'man' (verb)\")\n",
        "        print(\"     - Meaning: Elderly people operate the boat\")\n",
        "\n",
        "# ========== Implementation Demonstration ==========\n",
        "print(\"\\n\\n\" + \"=\" * 60)\n",
        "print(\"IMPLEMENTATION: SIMPLE DISAMBIGUATION SYSTEM\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def analyze_sentence(sentence):\n",
        "    \"\"\"Simple function to analyze sentence ambiguity\"\"\"\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    print(f\"\\nAnalyzing: '{sentence}'\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"POS tags: {tags}\")\n",
        "\n",
        "    # Check for specific patterns\n",
        "    if \"flies\" in sentence.lower() and \"like\" in sentence.lower():\n",
        "        print(\"\\nPattern detected: 'X flies like Y'\")\n",
        "        print(\"Possible interpretations:\")\n",
        "        print(\"  1. X (flies like Y) - X moves similarly to Y\")\n",
        "        print(\"  2. X-flies (like Y) - Insects called X-flies enjoy Y\")\n",
        "\n",
        "    elif \"can\" in tokens and tokens.count(\"can\") > 1:\n",
        "        print(\"\\nPattern detected: Multiple 'can' usage\")\n",
        "        print(\"Common pattern: Modal verb + Verb + Determiner + Noun\")\n",
        "        print(\"Example: We [can-MD] [can-VB] the [can-NN]\")\n",
        "\n",
        "    return tokens, tags\n",
        "\n",
        "# Test the function\n",
        "test_sentences = [\n",
        "    \"Time flies like an arrow\",\n",
        "    \"We can can the can\",\n",
        "    \"I saw the man with the telescope\"\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "    analyze_sentence(sent)\n",
        "\n",
        "# ========== Conclusion ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONCLUSION: POS TAGGING AND DISAMBIGUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nKey takeaways from this tutorial:\")\n",
        "print(\"\\n1. POS Tagging Methods:\")\n",
        "print(\"   - Rule-based: Transparent but limited\")\n",
        "print(\"   - Statistical: Learns from data, more robust\")\n",
        "print(\"   - Hybrid (Brill/HMM): Combines strengths\")\n",
        "\n",
        "print(\"\\n2. Ambiguity Types:\")\n",
        "print(\"   - Lexical: Same word, different meanings/POS\")\n",
        "print(\"   - Structural: Same words, different parse trees\")\n",
        "print(\"   - Scope: Modifier attachment problems\")\n",
        "\n",
        "print(\"\\n3. Language Differences:\")\n",
        "print(\"   - English: Rich morphology helps POS tagging\")\n",
        "print(\"   - Chinese: Relies more on word order and particles\")\n",
        "print(\"   - Both: Require context for full disambiguation\")\n",
        "\n",
        "print(\"\\n4. Practical Applications:\")\n",
        "print(\"   - Machine translation\")\n",
        "print(\"   - Information extraction\")\n",
        "print(\"   - Question answering\")\n",
        "print(\"   - Grammar checking\")\n",
        "\n",
        "print(\"\\n5. Limitations:\")\n",
        "print(\"   - POS tagging alone is insufficient\")\n",
        "print(\"   - Need syntax and semantics\")\n",
        "print(\"   - Context is crucial\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"END OF TUTORIAL ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Quick test of the tutorial's main example\n",
        "print(\"\\n\\nQUICK TEST: Tutorial's main example\")\n",
        "main_example = \"We can can the can\"\n",
        "tokens_ex = nltk.word_tokenize(main_example)\n",
        "tags_ex = nltk.pos_tag(tokens_ex)\n",
        "\n",
        "print(f\"\\nSentence: {main_example}\")\n",
        "print(\"Word-by-word analysis:\")\n",
        "for i, (word, tag) in enumerate(tags_ex):\n",
        "    pos_explanation = {\n",
        "        'PRP': 'Personal pronoun',\n",
        "        'MD': 'Modal verb (indicates ability/permission)',\n",
        "        'VB': 'Verb (base form)',\n",
        "        'DT': 'Determiner',\n",
        "        'NN': 'Noun (singular)'\n",
        "    }\n",
        "    explanation = pos_explanation.get(tag, 'Unknown')\n",
        "    print(f\"  '{word}': {tag} - {explanation}\")\n",
        "\n",
        "print(\"\\nComplete analysis:\")\n",
        "print(\"  'We can [can] the [can]'\")\n",
        "print(\"  Where:\")\n",
        "print(\"    can‚ÇÅ (position 2): Modal verb - 'are able to'\")\n",
        "print(\"    can‚ÇÇ (position 3): Verb - 'to put into containers'\")\n",
        "print(\"    can‚ÇÉ (position 5): Noun - 'metal container'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM5v6PykThyF",
        "outputId": "4eb6dbd0-b39f-468e-be41-ab9b085ba065"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.5/1.5 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h============================================================\n",
            "EXERCISE 1: English Ambiguity - 'Time flies like an arrow'\n",
            "============================================================\n",
            "Original sentence: Time flies like an arrow\n",
            "Tokenized: ['Time', 'flies', 'like', 'an', 'arrow']\n",
            "\n",
            "Method 1: Rule-based POS Tagging\n",
            "----------------------------------------\n",
            "Rule-based tagging results:\n",
            "  Time: NN\n",
            "  flies: VBZ\n",
            "  like: IN\n",
            "  an: DT\n",
            "  arrow: NN\n",
            "\n",
            "Method 2: NLTK Default Tagger (Transformation-based)\n",
            "----------------------------------------\n",
            "NLTK tagging results:\n",
            "  Time: NNP\n",
            "  flies: NNS\n",
            "  like: IN\n",
            "  an: DT\n",
            "  arrow: NN\n",
            "\n",
            "Method 3: Custom Pattern-based Tagger\n",
            "----------------------------------------\n",
            "Pattern-based tagging results:\n",
            "  Time: NN\n",
            "  flies: VBZ\n",
            "  like: NN\n",
            "  an: NN\n",
            "  arrow: NN\n",
            "\n",
            "============================================================\n",
            "ANALYSIS AND COMPARISON\n",
            "============================================================\n",
            "\n",
            "Tagging results comparison:\n",
            "Word     Rule-based   NLTK         Pattern-based\n",
            "---------------------------------------------\n",
            "Time     NN           NNP          NN          \n",
            "flies    VBZ          NNS          VBZ         \n",
            "like     IN           IN           NN          \n",
            "an       DT           DT           NN          \n",
            "arrow    NN           NN           NN          \n",
            "\n",
            "1. Key ambiguity points:\n",
            "   - 'flies': Can be verb (VBZ) or noun (NNS)\n",
            "   - 'like': Can be preposition (IN) or verb (VB)\n",
            "   - 'Time': Always noun (NN) in this context\n",
            "\n",
            "2. Two interpretations:\n",
            "   Interpretation A: 'Time passes quickly like an arrow'\n",
            "   - Structure: Time(NN) flies(VBZ) like(IN) an(DT) arrow(NN)\n",
            "   - Meaning: Time moves as fast as an arrow\n",
            "\n",
            "   Interpretation B: 'Time-flies (insects) enjoy an arrow'\n",
            "   - Structure: Time(NN) flies(NNS) like(VB) an(DT) arrow(NN)\n",
            "   - Meaning: Insects called 'time flies' like arrows\n",
            "\n",
            "3. Method evaluation:\n",
            "   - Rule-based: Good for specific cases, needs manual tuning\n",
            "   - NLTK: Most reliable, uses statistical and rule-based hybrid\n",
            "   - Pattern-based: Good fallback, uses word endings and patterns\n",
            "\n",
            "\n",
            "============================================================\n",
            "EXERCISE 2: Chinese Sentence Analysis\n",
            "============================================================\n",
            "NOTE: Chinese analysis requires additional libraries.\n",
            "We'll analyze the sentence conceptually instead.\n",
            "\n",
            "Chinese sentence: ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫\n",
            "Pinyin: W«í x«êhuƒÅn chƒ´ p√≠nggu«í de r√©n\n",
            "Literal: I like eat apple DE person\n",
            "Common translation: 'I like people who eat apples' or 'I like to eat apples'\n",
            "\n",
            "----------------------------------------\n",
            "MANUAL LINGUISTIC ANALYSIS\n",
            "----------------------------------------\n",
            "\n",
            "Word-by-word breakdown:\n",
            "  Êàë (w«í) - I/me (pronoun)\n",
            "  ÂñúÊ¨¢ (x«êhuƒÅn) - to like (verb)\n",
            "  ÂêÉ (chƒ´) - to eat (verb)\n",
            "  ËãπÊûú (p√≠nggu«í) - apple (noun)\n",
            "  ÁöÑ (de) - possessive/relative particle\n",
            "  ‰∫∫ (r√©n) - person/people (noun)\n",
            "\n",
            "Two possible structures:\n",
            "\n",
            "1. Structure A: Êàë + ÂñúÊ¨¢ + [ÂêÉËãπÊûú + ÁöÑ + ‰∫∫]\n",
            "   Parse: I + like + [people who eat apples]\n",
            "   Meaning: I like people who eat apples\n",
            "   POS: PRON + VERB + [VERB + NOUN + PART + NOUN]\n",
            "\n",
            "2. Structure B: Êàë + ÂñúÊ¨¢ + ÂêÉ + ËãπÊûú\n",
            "   Parse: I + like + eat + apples\n",
            "   Meaning: I like to eat apples ('ÁöÑ‰∫∫' is redundant)\n",
            "   POS: PRON + VERB + VERB + NOUN\n",
            "\n",
            "3. Structure C: [ÊàëÂñúÊ¨¢ÂêÉËãπÊûú] + ÁöÑ + ‰∫∫\n",
            "   Parse: [I like to eat apples] + 's + person\n",
            "   Meaning: The person who likes to eat apples\n",
            "   POS: [PRON + VERB + VERB + NOUN] + PART + NOUN\n",
            "\n",
            "Ambiguity source:\n",
            "  - The particle 'ÁöÑ' can create relative clauses\n",
            "  - Chinese allows noun phrases without explicit relative pronouns\n",
            "  - Context determines the correct parse\n",
            "\n",
            "============================================================\n",
            "COMPARISON: ENGLISH vs CHINESE AMBIGUITY\n",
            "============================================================\n",
            "\n",
            "English 'Time flies like an arrow':\n",
            "  - Type: Lexical and structural ambiguity\n",
            "  - Resolution: POS tagging + syntax parsing\n",
            "  - Key words: 'flies'(V/N), 'like'(P/V)\n",
            "\n",
            "Chinese 'ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫':\n",
            "  - Type: Structural/syntactic ambiguity\n",
            "  - Resolution: Phrase structure parsing\n",
            "  - Key element: Particle 'ÁöÑ' (creates relative clauses)\n",
            "\n",
            "Similarities:\n",
            "  - Both require context for full disambiguation\n",
            "  - Both have multiple valid interpretations\n",
            "  - Syntax plays crucial role in meaning\n",
            "\n",
            "Differences:\n",
            "  - English: Word-level ambiguity (same word, different POS)\n",
            "  - Chinese: Phrase-level ambiguity (same sequence, different structures)\n",
            "  - English uses word order and function words\n",
            "  - Chinese uses particles and context\n",
            "\n",
            "\n",
            "============================================================\n",
            "ADVANCED ANALYSIS: TUTORIAL EXAMPLES\n",
            "============================================================\n",
            "\n",
            "Testing additional ambiguous sentences:\n",
            "\n",
            "1. Sentence: 'We can can the can'\n",
            "   Tokens: ['We', 'can', 'can', 'the', 'can']\n",
            "   POS tags: [('We', 'PRP'), ('can', 'MD'), ('can', 'MD'), ('the', 'DT'), ('can', 'MD')]\n",
            "   Analysis: 'can' appears 3 times with different POS:\n",
            "     - Position 1: Modal verb (ability)\n",
            "     - Position 2: Verb (to preserve)\n",
            "     - Position 4: Noun (container)\n",
            "\n",
            "2. Sentence: 'Fruit flies like a banana'\n",
            "   Tokens: ['Fruit', 'flies', 'like', 'a', 'banana']\n",
            "   POS tags: [('Fruit', 'NNP'), ('flies', 'VBZ'), ('like', 'IN'), ('a', 'DT'), ('banana', 'NN')]\n",
            "   Analysis: Same ambiguity as 'Time flies like an arrow'\n",
            "     - 'flies': Can be noun (insects) or verb\n",
            "     - 'like': Can be preposition or verb\n",
            "\n",
            "3. Sentence: 'The old man the boat'\n",
            "   Tokens: ['The', 'old', 'man', 'the', 'boat']\n",
            "   POS tags: [('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]\n",
            "   Analysis: Garden path sentence\n",
            "     - Initial parse: 'The old man' (noun phrase)\n",
            "     - Correct parse: 'The old' (noun) + 'man' (verb)\n",
            "     - Meaning: Elderly people operate the boat\n",
            "\n",
            "\n",
            "============================================================\n",
            "IMPLEMENTATION: SIMPLE DISAMBIGUATION SYSTEM\n",
            "============================================================\n",
            "\n",
            "Analyzing: 'Time flies like an arrow'\n",
            "Tokens: ['Time', 'flies', 'like', 'an', 'arrow']\n",
            "POS tags: [('Time', 'NNP'), ('flies', 'NNS'), ('like', 'IN'), ('an', 'DT'), ('arrow', 'NN')]\n",
            "\n",
            "Pattern detected: 'X flies like Y'\n",
            "Possible interpretations:\n",
            "  1. X (flies like Y) - X moves similarly to Y\n",
            "  2. X-flies (like Y) - Insects called X-flies enjoy Y\n",
            "\n",
            "Analyzing: 'We can can the can'\n",
            "Tokens: ['We', 'can', 'can', 'the', 'can']\n",
            "POS tags: [('We', 'PRP'), ('can', 'MD'), ('can', 'MD'), ('the', 'DT'), ('can', 'MD')]\n",
            "\n",
            "Pattern detected: Multiple 'can' usage\n",
            "Common pattern: Modal verb + Verb + Determiner + Noun\n",
            "Example: We [can-MD] [can-VB] the [can-NN]\n",
            "\n",
            "Analyzing: 'I saw the man with the telescope'\n",
            "Tokens: ['I', 'saw', 'the', 'man', 'with', 'the', 'telescope']\n",
            "POS tags: [('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', 'NN')]\n",
            "\n",
            "============================================================\n",
            "CONCLUSION: POS TAGGING AND DISAMBIGUATION\n",
            "============================================================\n",
            "\n",
            "Key takeaways from this tutorial:\n",
            "\n",
            "1. POS Tagging Methods:\n",
            "   - Rule-based: Transparent but limited\n",
            "   - Statistical: Learns from data, more robust\n",
            "   - Hybrid (Brill/HMM): Combines strengths\n",
            "\n",
            "2. Ambiguity Types:\n",
            "   - Lexical: Same word, different meanings/POS\n",
            "   - Structural: Same words, different parse trees\n",
            "   - Scope: Modifier attachment problems\n",
            "\n",
            "3. Language Differences:\n",
            "   - English: Rich morphology helps POS tagging\n",
            "   - Chinese: Relies more on word order and particles\n",
            "   - Both: Require context for full disambiguation\n",
            "\n",
            "4. Practical Applications:\n",
            "   - Machine translation\n",
            "   - Information extraction\n",
            "   - Question answering\n",
            "   - Grammar checking\n",
            "\n",
            "5. Limitations:\n",
            "   - POS tagging alone is insufficient\n",
            "   - Need syntax and semantics\n",
            "   - Context is crucial\n",
            "\n",
            "============================================================\n",
            "END OF TUTORIAL ANALYSIS\n",
            "============================================================\n",
            "\n",
            "\n",
            "QUICK TEST: Tutorial's main example\n",
            "\n",
            "Sentence: We can can the can\n",
            "Word-by-word analysis:\n",
            "  'We': PRP - Personal pronoun\n",
            "  'can': MD - Modal verb (indicates ability/permission)\n",
            "  'can': MD - Modal verb (indicates ability/permission)\n",
            "  'the': DT - Determiner\n",
            "  'can': MD - Modal verb (indicates ability/permission)\n",
            "\n",
            "Complete analysis:\n",
            "  'We can [can] the [can]'\n",
            "  Where:\n",
            "    can‚ÇÅ (position 2): Modal verb - 'are able to'\n",
            "    can‚ÇÇ (position 3): Verb - 'to put into containers'\n",
            "    can‚ÇÉ (position 5): Noun - 'metal container'\n"
          ]
        }
      ]
    }
  ]
}