{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaoyuchen1515-art/rulinwaishi/blob/main/CBS5502_Tutorial_pos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sve2XAV8pT5"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# üß† **CBS5502 ‚Äî Computational Linguistics and NLP Technologies**\n",
        "\n",
        "### üêç **2nd Python Tutorial**\n",
        "### üìÖ *February 4, 2026*\n",
        "\n",
        "---\n",
        "\n",
        "## üá®üá≥ **PoS Tagging and Disambiguation**\n",
        "---\n",
        "### üë®‚Äçüè´ **Instructor**\n",
        "**Dr. WAN Mingyu**\n",
        "\n",
        "### üë®‚Äçüè´ **Teaching Assistant**\n",
        "**Mr. BAO Xiaoyi**\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "## üåü Welcome!\n",
        "\n",
        "Welcome to the tutorial series of **CBS5502**!  \n",
        "In this tutorial, we will explore how **Part-of-Speech (PoS) tagging** works and how ambiguity can be resolved using **three different approaches**, all demonstrated with the classic ambiguous sentence:\n",
        "\n",
        "> **‚ÄúWe can can the can.‚Äù** üåü\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "- Understand what **PoS tagging** is and why it is important in NLP  \n",
        "- Identify **lexical and structural ambiguity** in natural language  \n",
        "- Apply **three approaches to PoS tagging**:\n",
        "  - Rule-based tagging\n",
        "  - Statistical / probabilistic tagging\n",
        "  - Dictionary‚Äë or library‚Äëbased tagging using Python  \n",
        "- Analyze and interpret tagging results for ambiguous sentences  \n",
        "\n",
        "---\n",
        "\n",
        "üöÄ Let‚Äôs Get Started!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.tag import hmm\n",
        "from collections import defaultdict\n",
        "from nltk.tag import brill, brill_trainer\n",
        "from nltk.tag import UnigramTagger, BigramTagger, DefaultTagger\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Ensure you have the required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"We can can the can.\"\n",
        "\n",
        "# Tokenizing the sentence into words\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(\"Tokenized Sentence:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "KKMAAcHjeUey",
        "outputId": "94592ed3-66fa-4264-fb1c-e1392872f644"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1155771201.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import required libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt_tab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2479\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2480\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1Ô∏è‚É£ Rule‚ÄëBased Approach\n",
        "\n",
        "#### üîç Overview\n",
        "The **rule‚Äëbased approach** assigns Part‚Äëof‚ÄëSpeech (PoS) tags using **handcrafted linguistic rules**, typically based on word forms, surrounding context, or fixed patterns.  \n",
        "This method does **not rely on training data**, making it easy to understand and implement.\n",
        "\n",
        "#### üß† How It Works\n",
        "- Each rule matches a word (or pattern) in the sentence\n",
        "- The **first matching rule** determines the PoS tag\n",
        "- Rules are applied **sequentially**, from top to bottom\n",
        "\n",
        "#### üß© Example Rules\n",
        "For our ambiguous sentence, we define a few **simple and intuitive rules**:\n",
        "- Tag **‚ÄúWe‚Äù** as a personal pronoun\n",
        "- Tag **‚Äúthe‚Äù** as a determiner\n",
        "- Assign **‚Äúcan‚Äù** a default modal‚Äëverb tag\n",
        "- Use a fallback rule for unknown cases\n",
        "\n",
        "These rules illustrate both the **strength** (clarity) and **limitation** (lack of context awareness) of the rule‚Äëbased approach."
      ],
      "metadata": {
        "id": "6dzn8wQkeyDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# STEP 1: Define default (most likely) POS tags\n",
        "# --------------------------------------------------\n",
        "# This dictionary provides a fallback tag for each word.\n",
        "# If no contextual rule applies, we use these tags.\n",
        "most_likely_tags = {\n",
        "    \"We\": \"PRP\",   # Personal pronoun\n",
        "    \"can\": \"MD\",   # Modal verb (default assumption)\n",
        "    \"the\": \"DT\"    # Determiner\n",
        "}"
      ],
      "metadata": {
        "id": "0YwOzmcwi_Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# STEP 2: Define the rule-based POS tagging function\n",
        "# --------------------------------------------------\n",
        "def rule_based_pos_tagger(tokens):\n",
        "    \"\"\"\n",
        "    Assign POS tags to a list of tokens using\n",
        "    handcrafted contextual rules.\n",
        "\n",
        "    Parameters:\n",
        "        tokens (list): A list of word tokens\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (word, POS tag) tuples\n",
        "    \"\"\"\n",
        "\n",
        "    tagged_sentence = []  # Store the final tagged output\n",
        "\n",
        "    # Iterate through each word with its position\n",
        "    for i, word in enumerate(tokens):\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # STEP 3: Apply context-sensitive rules\n",
        "        # --------------------------------------------------\n",
        "\n",
        "        # Rule 1:\n",
        "        # If \"can\" appears immediately after \"We\",\n",
        "        # it functions as a modal verb (e.g., \"We can ...\")\n",
        "        if word == \"can\" and i > 0 and tokens[i - 1] == \"We\":\n",
        "            tag = \"MD\"\n",
        "\n",
        "        # Rule 2:\n",
        "        # If \"can\" follows \"the\", it is treated as a noun\n",
        "        # (e.g., \"the can\")\n",
        "        elif word == \"can\" and i > 0 and tokens[i - 1] == \"the\":\n",
        "            tag = \"NN\"\n",
        "\n",
        "        # Rule 3:\n",
        "        # If \"can\" follows another \"can\",\n",
        "        # it is treated as a main verb\n",
        "        # (e.g., \"can can the...\")\n",
        "        elif word == \"can\" and i > 0 and tokens[i - 1] == \"can\":\n",
        "            tag = \"VB\"\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # STEP 4: Apply default rule\n",
        "        # --------------------------------------------------\n",
        "        # If no specific contextual rule matches,\n",
        "        # fall back to the most likely tag\n",
        "        else:\n",
        "            tag = most_likely_tags.get(word, \"NN\")\n",
        "            # Unknown words default to NN (noun)\n",
        "\n",
        "        # Add the (word, tag) pair to the result\n",
        "        tagged_sentence.append((word, tag))\n",
        "\n",
        "    return tagged_sentence"
      ],
      "metadata": {
        "id": "8xlieTqzjD2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîé Step‚Äëby‚ÄëStep Rule Application\n",
        "\n",
        "Sentence: **We can can the can .**\n",
        "\n",
        "| Position | Token | Left Context | Applied Rule | Assigned Tag |\n",
        "|---------:|-------|--------------|--------------|--------------|\n",
        "| 0 | We | ‚Äî | Default dictionary rule | PRP |\n",
        "| 1 | can | We | Rule 1: *can* after *We* | MD |\n",
        "| 2 | can | can | Rule 3: *can* after *can* | VB |\n",
        "| 3 | the | can | Default dictionary rule | DT |\n",
        "| 4 | can | the | Rule 2: *can* after *the* | NN |\n",
        "| 5 | . | can | Default fallback | NN |"
      ],
      "metadata": {
        "id": "r9blpkyokByy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------\n",
        "# STEP 5: Apply the rule-based tagger\n",
        "# --------------------------------------------------\n",
        "rule_based_tags = rule_based_pos_tagger(tokens)\n",
        "\n",
        "# Display the result\n",
        "print(\"Rule-Based POS Tags:\")\n",
        "for word, tag in rule_based_tags:\n",
        "    print(f\"{word:>5}  ‚Üí  {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Na_Cu88ijcEi",
        "outputId": "915558b9-7342-46e2-a6d8-baea197acc1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rule_based_pos_tagger' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1517147727.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# STEP 5: Apply the rule-based tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# --------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrule_based_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule_based_pos_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rule_based_pos_tagger' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ö†Ô∏è Error Cases & Discussion\n",
        "\n",
        "### Example 1\n",
        "Sentence: **They can fish.**\n",
        "\n",
        "Expected:\n",
        "- can ‚Üí MD\n",
        "- fish ‚Üí VB\n",
        "\n",
        "Rule-Based Output:\n",
        "- can ‚Üí MD ‚úÖ\n",
        "- fish ‚Üí NN ‚ùå\n",
        "\n",
        "üìå *Why?*  \n",
        "The system lacks a rule recognizing **verb usage without ‚Äúthe‚Äù**."
      ],
      "metadata": {
        "id": "2V9TJjNUkXlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2\n",
        "Sentence: **The can can rust.**\n",
        "\n",
        "Correct interpretation:\n",
        "- can ‚Üí NN\n",
        "- can ‚Üí VB\n",
        "\n",
        "Rule-Based Output:\n",
        "- can ‚Üí NN ‚úÖ\n",
        "- can ‚Üí VB ‚úÖ (by coincidence)\n",
        "\n",
        "üìå *Discussion point:*  \n",
        "Correct tagging here is **accidental**, not robust."
      ],
      "metadata": {
        "id": "yWPylpDfkeT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Teaching Notes\n",
        "\n",
        "- This approach relies entirely on **manually written rules**\n",
        "- Each rule encodes **explicit linguistic intuition**\n",
        "\n",
        "#### ‚úÖ Strengths\n",
        "- Easy to understand and interpret\n",
        "- Transparent decision‚Äëmaking process\n",
        "\n",
        "#### ‚ùå Limitations\n",
        "- Difficult to scale to large vocabularies\n",
        "- Brittle when encountering unseen or unexpected patterns\n",
        "\n",
        "### üß† Learning Takeaways\n",
        "\n",
        "- Rule-based tagging makes **linguistic assumptions explicit**\n",
        "- Context helps, but only when **manually encoded**\n",
        "- Error cases reveal why **learning from data is necessary**\n",
        "- HMM and Brill taggers automate what rules attempt to approximate"
      ],
      "metadata": {
        "id": "gqJ-_KYZjrv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Hidden Markov Model (HMM) Approach\n",
        "\n",
        "### üîç Overview\n",
        "The **Hidden Markov Model (HMM)** approach is a **statistical sequence‚Äëlabeling method** that assigns PoS tags by modeling language as a **probabilistic process**.  \n",
        "It predicts the **most likely sequence of tags** for a sentence using probabilities learned from a **tagged corpus**.\n",
        "\n",
        "### üß† Core Assumptions\n",
        "HMM PoS tagging relies on two key assumptions:\n",
        "\n",
        "- **Markov Assumption:**  \n",
        "  The current tag depends only on a limited number of previous tags (typically one or two).\n",
        "- **Output Independence Assumption:**  \n",
        "  Each word is generated independently given its tag.\n",
        "\n",
        "### üîÅ Decoding Strategy\n",
        "To determine the optimal tag sequence, HMMs use the **Viterbi algorithm**, which efficiently finds:\n",
        "\n",
        "> ‚úÖ The most probable tag sequence for the entire sentence,  \n",
        "> rather than tagging each word independently.\n",
        "\n"
      ],
      "metadata": {
        "id": "nUKdbj5elX10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Step 1: What Does an HMM Model?\n",
        "\n",
        "An HMM models language with two probability components:\n",
        "\n",
        "1. **Transition Probability**\n",
        "   - $$P(t_i \\mid t_{i-1})$$  \n",
        "   - How likely one tag follows another\n",
        "\n",
        "2. **Emission Probability**\n",
        "   - $$P(w_i \\mid t_i)$$  \n",
        "   - How likely a word is generated by a tag\n",
        "\n",
        "The goal is to find the **most probable tag sequence** for the entire sentence."
      ],
      "metadata": {
        "id": "qJ0KW74sl_Lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ Step 2: Why We Need Sequence‚ÄëLevel Decisions\n",
        "\n",
        "Ambiguous words like **‚Äúcan‚Äù** cannot be tagged reliably in isolation.\n",
        "\n",
        "‚úÖ HMMs solve this by:\n",
        "- Considering **previous tags**\n",
        "- Evaluating the **entire sentence**\n",
        "- Using **global optimization** via the Viterbi algorithm"
      ],
      "metadata": {
        "id": "j58EFuGPmcm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Step 3: Environment Setup"
      ],
      "metadata": {
        "id": "zIdsEKEBmgT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import hmm"
      ],
      "metadata": {
        "id": "bX7yBkUYmia-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "kwEZzDc8ml6W",
        "outputId": "2e0a107b-6d08-47a4-99c0-e2595c99cf6c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nltk' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-893122810.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download resources (run once)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'brown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìö Step 4: Prepare Training Data\n",
        "\n",
        "HMMs require a **tagged corpus** to learn probabilities.\n",
        "Here, we use the Brown Corpus (news category)."
      ],
      "metadata": {
        "id": "b-Kf0joDmpoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üè∑Ô∏è Official Tag List for `hmm_tagger` (Brown Tagset)\n",
        "\n",
        "### ‚úÖ Primary Reference (Recommended for Teaching)\n",
        "\n",
        "You can find the **complete Brown Corpus tagset**, along with detailed explanations, in the **NLTK Book, Chapter 5**:\n",
        "\n",
        "üëâ **NLTK Book ‚Äî Categorizing and Tagging Words (Brown Tagset)**  \n",
        "*(nltk.org)*\n",
        "\n",
        "### üìö What This Section Covers\n",
        "\n",
        "This reference documents:\n",
        "\n",
        "- ‚úÖ All Brown tags (e.g. `PPSS`, `AT`, `NP`, `VB`, `MD`)\n",
        "- ‚úÖ Examples of words annotated with each tag\n",
        "- ‚úÖ Key differences between the **Brown tagset** and the **Penn Treebank tagset**\n",
        "\n",
        "üìå **Teaching note:**  \n",
        "The `HiddenMarkovModelTagger` in NLTK inherits its tagset directly from the corpus it is trained on. When trained with the Brown Corpus, it therefore produces **Brown-style PoS tags**."
      ],
      "metadata": {
        "id": "PG-zVJOKoNcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tagged sentences for training\n",
        "train_sentences = brown.tagged_sents(categories='news')\n",
        "\n",
        "# Inspect one example\n",
        "train_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "BF_4HDGcmuQg",
        "outputId": "f80c20ca-2516-492b-d981-895aa0a0d8bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'brown' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3262031457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load tagged sentences for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'news'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Inspect one example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'brown' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üèóÔ∏è Step 5: Train the HMM Tagger\n",
        "\n",
        "The training process automatically learns:\n",
        "- Tag transition probabilities\n",
        "- Word emission probabilities"
      ],
      "metadata": {
        "id": "wXlCY1mtm0An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an HMM tagger\n",
        "hmm_tagger = hmm.HiddenMarkovModelTagger.train(train_sentences)"
      ],
      "metadata": {
        "id": "yyHLA67Sm23X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "3f3f6051-83b5-4de2-813e-4a4c5934cd71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hmm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4212796608.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train an HMM tagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhmm_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHiddenMarkovModelTagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'hmm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Step 6: Apply the HMM to an Ambiguous Sentence"
      ],
      "metadata": {
        "id": "60KICVVfm6Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"We can can the can .\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "hmm_tags = hmm_tagger.tag(tokens)\n",
        "hmm_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "Z0OC2snFm8H3",
        "outputId": "47fb75ca-201c-4f56-e0fb-ea9bbefa16cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nltk' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1906184146.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"We can can the can .\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhmm_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhmm_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úî The HMM correctly captures grammatical structure  \n",
        "‚úî Ambiguity is resolved using contextual probabilities  \n",
        "‚úî The same word can have different tags in the same sentence"
      ],
      "metadata": {
        "id": "J5fmq__PndPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Transformation‚ÄëBased (Brill) Tagging\n",
        "\n",
        "### üîç Overview\n",
        "The **Transformation‚ÄëBased Approach**, also known as **Brill Tagging**, is a **hybrid method** that combines:\n",
        "\n",
        "- ‚úÖ A **simple statistical baseline tagger**\n",
        "- ‚úÖ A set of **learned transformation rules**\n",
        "\n",
        "Instead of assigning tags in one step, Brill tagging **iteratively corrects errors** made by an initial tagger using rules learned from a **tagged corpus**.\n"
      ],
      "metadata": {
        "id": "7yMelAMli9kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Core Idea\n",
        "\n",
        "Brill tagging follows this learning cycle:\n",
        "\n",
        "1. Start with a **baseline tagger** (e.g., Unigram Tagger)\n",
        "2. Compare its output with **gold‚Äëstandard tags**\n",
        "3. Learn **transformation rules** that reduce errors\n",
        "4. Apply the rules sequentially to improve tagging accuracy\n",
        "\n",
        "üìå The learned rules are **human‚Äëreadable**, making this approach both **accurate and interpretable**."
      ],
      "metadata": {
        "id": "Ga95uh8Io8pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Step 1: Environment Setup"
      ],
      "metadata": {
        "id": "6ay9cDUOpEsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sklearn-crfsuite"
      ],
      "metadata": {
        "id": "zps22o2LpGV1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences = brown.tagged_sents(categories='news')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "w_N-oahorEnl",
        "outputId": "b9d69e3d-4e81-4598-e3a3-efe2682bee99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1481151338.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'brown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2479\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2480\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import DefaultTagger, UnigramTagger\n",
        "\n",
        "default_tagger = DefaultTagger('NN')\n",
        "baseline_tagger = UnigramTagger(sentences, backoff=default_tagger)"
      ],
      "metadata": {
        "id": "uG6RK34hrIUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"We can can the can .\".split()\n",
        "baseline_tagger.tag(sentence)"
      ],
      "metadata": {
        "id": "kolHV1FBrMG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word.lower()': word.lower(),\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        features['prev_word'] = sent[i-1]\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        features['next_word'] = sent[i+1]\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "yt52IRuNrPW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2features(sent):\n",
        "    return [word_features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [tag for _, tag in sent]\n",
        "\n",
        "X = [sent2features([w for w, t in s]) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]"
      ],
      "metadata": {
        "id": "bNhXzEB1rTCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_subset = sentences[:500]\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    max_iterations=10,\n",
        "    all_possible_transitions=False\n",
        ")\n",
        "\n",
        "crf.fit(X_small[:500], y_small[:500])"
      ],
      "metadata": {
        "id": "zAEVZpZIrW2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"We can can the can .\".split()\n",
        "features = sent2features(test_sentence)\n",
        "\n",
        "list(zip(test_sentence, crf.predict_single(features)))"
      ],
      "metadata": {
        "id": "rdzWfDsgrdDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö†Ô∏è Practical Note on CRF Training\n",
        "\n",
        "CRFs are powerful but computationally expensive models.\n",
        "Training on large corpora with all possible tag transitions\n",
        "can take a very long time.\n",
        "\n",
        "‚úÖ For teaching and experimentation:\n",
        "- Use a small subset of data\n",
        "- Limit the number of iterations\n",
        "- Disable unnecessary transitions\n",
        "\n",
        "This preserves the learning behavior while keeping runtime manageable."
      ],
      "metadata": {
        "id": "FmQnDz9qwm4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üß† Interpretation\n",
        "\n",
        "- **Rule‚ÄëBased Tagger**\n",
        "  - Correctly handles this sentence due to carefully designed rules.\n",
        "  - Performance is **fragile** and depends entirely on manual rule coverage.\n",
        "\n",
        "- **Hidden Markov Model (HMM)**\n",
        "  - Resolves ambiguity using **learned transition and emission probabilities**.\n",
        "  - Makes **global sequence‚Äëlevel decisions**, leading to robust results.\n",
        "\n",
        "- **Brill‚ÄëStyle (Transformation‚ÄëBased) Tagger**\n",
        "  - Starts from a weak baseline and **learns contextual corrections**.\n",
        "  - Combines the **interpretability of rules** with **data‚Äëdriven learning**.\n",
        "  - Often outperforms unigram or bigram taggers when **training data is limited**.\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Key Takeaway\n",
        "\n",
        "Although all three methods succeed on this example, they do so for different reasons:\n",
        "\n",
        "- Rule‚Äëbased tagging relies on **explicit linguistic intuition**\n",
        "- HMM tagging relies on **probabilistic sequence modeling**\n",
        "- Brill‚Äëstyle tagging bridges both worlds by **learning rules from data**\n",
        "\n",
        "This comparison highlights why transformation‚Äëbased methods remain an important conceptual bridge between symbolic and statistical NLP approaches."
      ],
      "metadata": {
        "id": "DnDNJW0QkdBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Playground ‚Äî Ending Exercises\n",
        "\n",
        "The following exercises encourage you to **apply, compare, and reflect** on the three PoS tagging approaches covered in this tutorial. Focus on **ambiguity**, **context**, and **model behavior** rather than just correctness.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Exercise 1: English Ambiguity Challenge  \n",
        "**Sentence:**  \n",
        "> *‚ÄúTime flies like an arrow.‚Äù*\n",
        "\n",
        "This sentence is famously ambiguous and can be interpreted in multiple ways.\n",
        "\n",
        "#### ‚úÖ Tasks\n",
        "1. **Tokenize** the sentence.\n",
        "2. Apply:\n",
        "   - Rule‚Äëbased tagging  \n",
        "   - HMM tagging  \n",
        "   - Brill‚Äëstyle (transformation‚Äëbased) tagging\n",
        "3. Record the PoS tags produced by each method.\n",
        "\n",
        "#### üß† Guiding Questions\n",
        "- Which word(s) show different PoS tags across methods?\n",
        "- Does *flies* behave as a **noun** or a **verb**?\n",
        "- Is *like* treated as a **verb**, **preposition**, or **conjunction**?\n",
        "- Which approach best captures the intended reading?\n",
        "\n",
        "#### üí° Reflection\n",
        "- Why is this sentence difficult to tag correctly without full syntactic analysis?\n",
        "- How does sequence‚Äëlevel modeling help resolve ambiguity?\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Exercise 2: Chinese Structural Ambiguity  \n",
        "**Sentence:**  \n",
        "> **ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫„ÄÇ**  \n",
        "> *(Pinyin: W«í x«êhuƒÅn chƒ´ p√≠nggu«í de r√©n.)*\n",
        "\n",
        "This sentence is a classic example used in **Chinese NLP** to test ambiguity resolution.\n",
        "\n",
        "#### ‚úÖ Tasks\n",
        "1. Segment the sentence into words (use a Chinese tokenizer if available).\n",
        "2. Assign PoS tags to each word.\n",
        "3. Identify at least **two possible interpretations** of the sentence.\n",
        "\n",
        "#### üß† Key Points to Consider\n",
        "- The grammatical role of **‚ÄúÁöÑ‚Äù**\n",
        "- Whether **‚ÄúÂêÉËãπÊûú‚Äù** modifies:\n",
        "  - *Êàë* (I like to eat apples), or\n",
        "  - *‰∫∫* (people who eat apples)\n",
        "- How relative clauses are formed in Chinese\n",
        "\n",
        "#### üí° Reflection\n",
        "- Why is **‚ÄúÁöÑ‚Äù** challenging for PoS tagging and parsing?\n",
        "- What additional information (syntax, semantics, or context) would help disambiguate the sentence?\n",
        "- Why do purely rule‚Äëbased approaches struggle with this example?\n",
        "\n",
        "---\n",
        "\n",
        "### üåü Take‚ÄëHome Insight\n",
        "\n",
        "These exercises illustrate that:\n",
        "\n",
        "- **PoS tagging alone is often insufficient** for full disambiguation\n",
        "- Ambiguity exists at both **lexical** and **structural** levels\n",
        "- Real‚Äëworld NLP systems must integrate **context, syntax, and semantics**\n",
        "\n",
        "‚úÖ Congratulations on completing the tutorial!"
      ],
      "metadata": {
        "id": "nNuN6ogTgsZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall nltk -y\n",
        "!pip uninstall sklearn-crfsuite -y\n",
        "!pip uninstall jieba -y\n",
        "!pip uninstall pkuseg -y\n",
        "\n",
        "# Ê∏ÖÁêÜ pip ÁºìÂ≠ò\n",
        "!pip cache purge\n",
        "\n",
        "# ÂÆâË£ÖÊâÄÊúâÈúÄË¶ÅÁöÑÂ∫ì\n",
        "!pip install nltk==3.8.1\n",
        "!pip install numpy\n",
        "!pip install jieba\n",
        "\n",
        "print(\"ÂÆâË£ÖÂÆåÊàêÔºÅÁé∞Âú®ÈáçÊñ∞ÂêØÂä®ËøêË°åÊó∂...\")"
      ],
      "metadata": {
        "id": "dPA_s1MxKiNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, reinstall nltk to fix any issues\n",
        "!pip uninstall nltk -y -q\n",
        "!pip install nltk==3.8.1 -q\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('brown', quiet=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXERCISE 1: English Ambiguity - 'Time flies like an arrow'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define the sentence\n",
        "sentence = \"Time flies like an arrow\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(f\"Original sentence: {sentence}\")\n",
        "print(f\"Tokenized: {tokens}\\n\")\n",
        "\n",
        "# ========== Method 1: Rule-based Tagging ==========\n",
        "print(\"Method 1: Rule-based POS Tagging\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Define tagging rules\n",
        "rule_based_tags = []\n",
        "\n",
        "for i, word in enumerate(tokens):\n",
        "    word_lower = word.lower()\n",
        "\n",
        "    # Rule 1: First word \"Time\" is likely a noun\n",
        "    if i == 0 and word_lower == \"time\":\n",
        "        rule_based_tags.append((word, \"NN\"))  # Noun\n",
        "\n",
        "    # Rule 2: \"flies\" could be verb or noun\n",
        "    elif word_lower == \"flies\":\n",
        "        # Check previous word\n",
        "        if i > 0 and tokens[i-1].lower() == \"time\":\n",
        "            # \"Time flies\" - flies is likely a verb\n",
        "            rule_based_tags.append((word, \"VBZ\"))  # Verb 3rd person singular\n",
        "        else:\n",
        "            rule_based_tags.append((word, \"NNS\"))  # Noun plural\n",
        "\n",
        "    # Rule 3: \"like\" could be preposition or verb\n",
        "    elif word_lower == \"like\":\n",
        "        # Check context\n",
        "        if i > 0 and tokens[i-1].lower() in [\"flies\", \"fly\"]:\n",
        "            rule_based_tags.append((word, \"IN\"))  # Preposition\n",
        "        else:\n",
        "            rule_based_tags.append((word, \"VB\"))  # Verb\n",
        "\n",
        "    # Rule 4: \"an\" is a determiner\n",
        "    elif word_lower == \"an\":\n",
        "        rule_based_tags.append((word, \"DT\"))\n",
        "\n",
        "    # Rule 5: \"arrow\" is a noun\n",
        "    elif word_lower == \"arrow\":\n",
        "        rule_based_tags.append((word, \"NN\"))\n",
        "\n",
        "    # Default rule\n",
        "    else:\n",
        "        rule_based_tags.append((word, \"UNK\"))  # Unknown\n",
        "\n",
        "print(\"Rule-based tagging results:\")\n",
        "for word, tag in rule_based_tags:\n",
        "    print(f\"  {word}: {tag}\")\n",
        "\n",
        "# ========== Method 2: NLTK Default Tagger ==========\n",
        "print(\"\\nMethod 2: NLTK Default Tagger (Transformation-based)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Use NLTK's pre-trained tagger\n",
        "default_tags = nltk.pos_tag(tokens)\n",
        "print(\"NLTK tagging results:\")\n",
        "for word, tag in default_tags:\n",
        "    print(f\"  {word}: {tag}\")\n",
        "\n",
        "# ========== Method 3: Statistical Tagging ==========\n",
        "print(\"\\nMethod 3: Statistical Tagging (Based on Brown Corpus)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Create frequency distributions from Brown Corpus\n",
        "tag_freq = nltk.FreqDist(tag for (word, tag) in brown.tagged_words())\n",
        "word_tag_freq = nltk.ConditionalFreqDist(brown.tagged_words())\n",
        "\n",
        "# Create statistical tagger\n",
        "statistical_tags = []\n",
        "for word in tokens:\n",
        "    word_lower = word.lower()\n",
        "\n",
        "    # If word exists in corpus, use most frequent tag\n",
        "    if word_lower in word_tag_freq:\n",
        "        most_common_tag = word_tag_freq[word_lower].max()\n",
        "        statistical_tags.append((word, most_common_tag))\n",
        "    else:\n",
        "        # Fallback strategy\n",
        "        if word_lower in [\"time\", \"arrow\"]:\n",
        "            statistical_tags.append((word, \"NN\"))\n",
        "        elif word_lower == \"flies\":\n",
        "            statistical_tags.append((word, \"NNS\"))  # Default as plural noun\n",
        "        elif word_lower == \"like\":\n",
        "            statistical_tags.append((word, \"IN\"))  # Default as preposition\n",
        "        elif word_lower == \"an\":\n",
        "            statistical_tags.append((word, \"AT\"))  # Article tag in Brown Corpus\n",
        "        else:\n",
        "            statistical_tags.append((word, \"UNK\"))\n",
        "\n",
        "print(\"Statistical tagging results:\")\n",
        "for word, tag in statistical_tags:\n",
        "    print(f\"  {word}: {tag}\")\n",
        "\n",
        "# ========== Analysis and Comparison ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ANALYSIS AND COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convert to dictionaries for easy access\n",
        "rule_dict = dict(rule_based_tags)\n",
        "default_dict = dict(default_tags)\n",
        "stat_dict = dict(statistical_tags)\n",
        "\n",
        "print(\"\\n1. Tagging differences for 'flies':\")\n",
        "print(f\"   Rule-based: {rule_dict.get('flies', 'N/A')} - could be noun(NNS) or verb(VBZ)\")\n",
        "print(f\"   NLTK: {default_dict.get('flies', 'N/A')}\")\n",
        "print(f\"   Statistical: {stat_dict.get('flies', 'N/A')}\")\n",
        "\n",
        "print(\"\\n2. Tagging differences for 'like':\")\n",
        "print(f\"   Rule-based: {rule_dict.get('like', 'N/A')} - could be preposition(IN) or verb(VB)\")\n",
        "print(f\"   NLTK: {default_dict.get('like', 'N/A')}\")\n",
        "print(f\"   Statistical: {stat_dict.get('like', 'N/A')}\")\n",
        "\n",
        "print(\"\\n3. Two possible interpretations of the sentence:\")\n",
        "print(\"   a) Time passes quickly like an arrow (Time [flies like an arrow])\")\n",
        "print(\"      Structure: Noun + Verb phrase\")\n",
        "print(\"      Tagging: Time(NN) flies(VBZ) like(IN) an(DT) arrow(NN)\")\n",
        "print(\"      Meaning: Time passes as quickly as an arrow flies\")\n",
        "print(\"\\n   b) Time-flies (insects) enjoy an arrow (Time flies [like an arrow])\")\n",
        "print(\"      Structure: Noun phrase + Verb + Noun phrase\")\n",
        "print(\"      Tagging: Time(NN) flies(NNS) like(VB) an(DT) arrow(NN)\")\n",
        "print(\"      Meaning: Insects called 'time flies' enjoy arrows\")\n",
        "\n",
        "print(\"\\n4. Which method is best?\")\n",
        "print(\"   - Rule-based: Flexible but subjective\")\n",
        "print(\"   - NLTK: Based on extensive training data, usually most accurate\")\n",
        "print(\"   - Statistical: Based on corpus frequencies, needs sufficient data\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n\" + \"=\" * 60)\n",
        "print(\"EXERCISE 2: Chinese Ambiguity - 'ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install Chinese processing library\n",
        "try:\n",
        "    import jieba\n",
        "    print(\"jieba is already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing jieba...\")\n",
        "    !pip install jieba -q\n",
        "    import jieba\n",
        "\n",
        "# Chinese sentence\n",
        "chinese_sentence = \"ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫\"\n",
        "\n",
        "print(f\"Chinese sentence: {chinese_sentence}\")\n",
        "print(f\"Pinyin: W«í x«êhuƒÅn chƒ´ p√≠nggu«í de r√©n\")\n",
        "print(f\"Literal translation: I like eat apple DE person\")\n",
        "\n",
        "# ========== Method 1: jieba Segmentation ==========\n",
        "print(\"\\nMethod 1: jieba Word Segmentation\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Use jieba for word segmentation\n",
        "words = jieba.lcut(chinese_sentence)\n",
        "print(f\"Segmentation results: {words}\")\n",
        "\n",
        "# Manual POS tagging for jieba results\n",
        "jieba_pos = []\n",
        "for word in words:\n",
        "    if word == \"Êàë\":\n",
        "        jieba_pos.append((word, \"r\"))  # Pronoun\n",
        "    elif word == \"ÂñúÊ¨¢\":\n",
        "        jieba_pos.append((word, \"v\"))  # Verb\n",
        "    elif word == \"ÂêÉ\":\n",
        "        jieba_pos.append((word, \"v\"))  # Verb\n",
        "    elif word == \"ËãπÊûú\":\n",
        "        jieba_pos.append((word, \"n\"))  # Noun\n",
        "    elif word == \"ÁöÑ\":\n",
        "        jieba_pos.append((word, \"uj\"))  # Auxiliary\n",
        "    elif word == \"‰∫∫\":\n",
        "        jieba_pos.append((word, \"n\"))  # Noun\n",
        "    else:\n",
        "        jieba_pos.append((word, \"x\"))  # Unknown\n",
        "\n",
        "print(\"POS tagging results:\")\n",
        "for word, tag in jieba_pos:\n",
        "    print(f\"  {word}: {tag}\")\n",
        "\n",
        "print(\"\\nTag meanings:\")\n",
        "print(\"  r - pronoun\")\n",
        "print(\"  v - verb\")\n",
        "print(\"  n - noun\")\n",
        "print(\"  uj - auxiliary particle\")\n",
        "\n",
        "# ========== Method 2: Rule-based Analysis ==========\n",
        "print(\"\\nMethod 2: Rule-based Segmentation and POS Tagging\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Three possible segmentation methods\n",
        "segmentations = [\n",
        "    # Segmentation 1: Êàë / ÂñúÊ¨¢ / ÂêÉËãπÊûú / ÁöÑ / ‰∫∫\n",
        "    [\"Êàë\", \"ÂñúÊ¨¢\", \"ÂêÉËãπÊûú\", \"ÁöÑ\", \"‰∫∫\"],\n",
        "    # Segmentation 2: ÊàëÂñúÊ¨¢ / ÂêÉ / ËãπÊûú / ÁöÑ / ‰∫∫\n",
        "    [\"ÊàëÂñúÊ¨¢\", \"ÂêÉ\", \"ËãπÊûú\", \"ÁöÑ\", \"‰∫∫\"],\n",
        "    # Segmentation 3: Êàë / ÂñúÊ¨¢ / ÂêÉËãπÊûúÁöÑ‰∫∫\n",
        "    [\"Êàë\", \"ÂñúÊ¨¢\", \"ÂêÉËãπÊûúÁöÑ‰∫∫\"]\n",
        "]\n",
        "\n",
        "print(\"Three possible segmentation methods:\")\n",
        "for i, seg in enumerate(segmentations, 1):\n",
        "    print(f\"  Method {i}: {' | '.join(seg)}\")\n",
        "\n",
        "# Analyze first segmentation method\n",
        "selected_seg = segmentations[0]\n",
        "rule_pos_chinese = []\n",
        "\n",
        "# Assign POS tags\n",
        "for word in selected_seg:\n",
        "    if word == \"Êàë\":\n",
        "        rule_pos_chinese.append((word, \"PRON\"))\n",
        "    elif word == \"ÂñúÊ¨¢\":\n",
        "        rule_pos_chinese.append((word, \"VERB\"))\n",
        "    elif word == \"ÂêÉËãπÊûú\":\n",
        "        rule_pos_chinese.append((word, \"VP\"))  # Verb phrase\n",
        "    elif word == \"ÁöÑ\":\n",
        "        rule_pos_chinese.append((word, \"PART\"))  # Particle\n",
        "    elif word == \"‰∫∫\":\n",
        "        rule_pos_chinese.append((word, \"NOUN\"))\n",
        "    else:\n",
        "        rule_pos_chinese.append((word, \"UNK\"))\n",
        "\n",
        "print(\"\\nRule-based POS tagging (using segmentation method 1):\")\n",
        "for word, tag in rule_pos_chinese:\n",
        "    print(f\"  {word}: {tag}\")\n",
        "\n",
        "# ========== Ambiguity Analysis ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"AMBIGUITY ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nTwo main interpretations:\")\n",
        "print(\"\\n1. Interpretation A: I like [people who eat apples]\")\n",
        "print(\"   Structure: I + like + [people who eat apples]\")\n",
        "print(\"   Meaning: I like people who eat apples\")\n",
        "print(\"   Grammatical analysis:\")\n",
        "print(\"     - 'ÂêÉËãπÊûúÁöÑ' modifies '‰∫∫' (forms noun phrase)\")\n",
        "print(\"     - 'ÁöÑ' functions as a structural particle\")\n",
        "print(\"     - 'ÂêÉËãπÊûúÁöÑ‰∫∫' is the object of 'ÂñúÊ¨¢'\")\n",
        "\n",
        "print(\"\\n2. Interpretation B: I like to eat apples\")\n",
        "print(\"   Structure: I + like + eat + apples\")\n",
        "print(\"   Meaning: I like eating apples ('ÁöÑ‰∫∫' might be redundant)\")\n",
        "print(\"   Grammatical analysis:\")\n",
        "print(\"     - 'ÂêÉËãπÊûú' is the object of 'ÂñúÊ¨¢'\")\n",
        "print(\"     - 'ÁöÑ' might indicate emphasis or colloquial expression\")\n",
        "\n",
        "print(\"\\n3. Interpretation C: [I like eating apples]'s person (less common)\")\n",
        "print(\"   Structure: [I like eating apples] + ÁöÑ + person\")\n",
        "print(\"   Meaning: The person who likes eating apples\")\n",
        "print(\"   Grammatical analysis:\")\n",
        "print(\"     - 'ÊàëÂñúÊ¨¢ÂêÉËãπÊûú' modifies '‰∫∫' as a relative clause\")\n",
        "\n",
        "print(\"\\nKey ambiguity points:\")\n",
        "print(\"   1. Grammatical role of 'ÁöÑ':\")\n",
        "print(\"      - Structural particle: connects modifier and head noun\")\n",
        "print(\"      - Might indicate possession or modification\")\n",
        "print(\"      - Could be a modal particle in some dialects\")\n",
        "print(\"\\n   2. Syntactic function of 'ÂêÉËãπÊûú':\")\n",
        "print(\"      - As modifier: modifies '‰∫∫'\")\n",
        "print(\"      - As object: direct object of 'ÂñúÊ¨¢'\")\n",
        "print(\"\\n   3. Structural hierarchy:\")\n",
        "print(\"      - (I like (people who eat apples))\")\n",
        "print(\"      - ((I like eating apples)'s person)\")\n",
        "\n",
        "print(\"\\nDisambiguation strategies:\")\n",
        "print(\"   1. Context: surrounding sentences\")\n",
        "print(\"   2. Intonation/stress: in spoken language\")\n",
        "print(\"   3. Syntactic parsing: full parse tree analysis\")\n",
        "print(\"   4. Semantic analysis: agent and patient roles of 'ÂñúÊ¨¢'\")\n",
        "print(\"   5. Pragmatic factors: discourse context and speaker intent\")\n",
        "\n",
        "# ========== Summary and Comparison ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUMMARY AND COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nEnglish sentence 'Time flies like an arrow':\")\n",
        "print(\"  - Ambiguity type: Structural + Lexical\")\n",
        "print(\"  - Key points: 'flies'(verb/noun), 'like'(preposition/verb)\")\n",
        "print(\"  - Method performance:\")\n",
        "print(\"     Rule-based: Flexible but requires manual rules\")\n",
        "print(\"     NLTK: Usually most accurate, based on large training data\")\n",
        "print(\"     Statistical: Needs sufficient corpus data\")\n",
        "\n",
        "print(\"\\nChinese sentence 'ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫':\")\n",
        "print(\"  - Ambiguity type: Structural + Elliptical\")\n",
        "print(\"  - Key points: Function of 'ÁöÑ', structural hierarchy\")\n",
        "print(\"  - Method performance:\")\n",
        "print(\"     jieba: Automatic segmentation but limited POS tagging\")\n",
        "print(\"     Rule-based: Requires linguistic knowledge\")\n",
        "\n",
        "print(\"\\nGeneral conclusions:\")\n",
        "print(\"  1. POS tagging is fundamental but insufficient for full disambiguation\")\n",
        "print(\"  2. Structural ambiguity requires syntactic analysis\")\n",
        "print(\"  3. Statistical methods generally outperform pure rule-based methods\")\n",
        "print(\"  4. Chinese processing is more complex than English\")\n",
        "print(\"  5. Context is crucial for disambiguation\")\n",
        "\n",
        "# ========== Additional Tests ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ADDITIONAL TEST SENTENCES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_sentences = [\n",
        "    \"We can can the can\",  # Example from tutorial\n",
        "    \"Fruit flies like a banana\",  # Classic ambiguity\n",
        "    \"The old man the boat\"  # Garden path sentence\n",
        "]\n",
        "\n",
        "for i, sent in enumerate(test_sentences, 1):\n",
        "    print(f\"\\n{i}. Test sentence: {sent}\")\n",
        "    tokens = nltk.word_tokenize(sent)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    print(f\"   Tokens: {tokens}\")\n",
        "    print(f\"   POS tags: {tags}\")\n",
        "\n",
        "    # Simple explanation\n",
        "    if i == 1:\n",
        "        print(\"   Ambiguity: 'can' can be modal verb(MD), verb(VB), or noun(NN)\")\n",
        "    elif i == 2:\n",
        "        print(\"   Ambiguity: 'flies' can be noun(fruit flies) or verb(flies)\")\n",
        "    elif i == 3:\n",
        "        print(\"   Ambiguity: 'man' can be noun(person) or verb(to operate)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CODE EXECUTION COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Final demonstration with tutorial example\n",
        "print(\"\\n\\n\" + \"=\" * 60)\n",
        "print(\"TUTORIAL EXAMPLE: 'We can can the can'\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tutorial_sentence = \"We can can the can\"\n",
        "tutorial_tokens = nltk.word_tokenize(tutorial_sentence)\n",
        "tutorial_tags = nltk.pos_tag(tutorial_tokens)\n",
        "\n",
        "print(f\"Sentence: {tutorial_sentence}\")\n",
        "print(f\"Tokens: {tutorial_tokens}\")\n",
        "print(f\"POS tags: {tutorial_tags}\")\n",
        "print(\"\\nExplanation of 'can' occurrences:\")\n",
        "print(\"  1. 'can' (position 1): Modal verb (MD) - indicates ability\")\n",
        "print(\"  2. 'can' (position 2): Verb (VB) - means to preserve/containerize\")\n",
        "print(\"  3. 'can' (position 4): Noun (NN) - refers to a container\")\n",
        "\n",
        "print(\"\\nThis demonstrates lexical ambiguity:\")\n",
        "print(\"  - Same word form 'can' has three different POS tags\")\n",
        "print(\"  - Context determines the correct interpretation\")\n",
        "print(\"  - Statistical models learn these patterns from training data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBtc8oFCPB56",
        "outputId": "d574e2bd-acfb-448a-caa5-0af672869128"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m============================================================\n",
            "EXERCISE 1: English Ambiguity - 'Time flies like an arrow'\n",
            "============================================================\n",
            "Original sentence: Time flies like an arrow\n",
            "Tokenized: ['Time', 'flies', 'like', 'an', 'arrow']\n",
            "\n",
            "Method 1: Rule-based POS Tagging\n",
            "----------------------------------------\n",
            "Rule-based tagging results:\n",
            "  Time: NN\n",
            "  flies: VBZ\n",
            "  like: IN\n",
            "  an: DT\n",
            "  arrow: NN\n",
            "\n",
            "Method 2: NLTK Default Tagger (Transformation-based)\n",
            "----------------------------------------\n",
            "NLTK tagging results:\n",
            "  Time: NNP\n",
            "  flies: NNS\n",
            "  like: IN\n",
            "  an: DT\n",
            "  arrow: NN\n",
            "\n",
            "Method 3: Statistical Tagging (Based on Brown Corpus)\n",
            "----------------------------------------\n",
            "Statistical tagging results:\n",
            "  Time: NN\n",
            "  flies: NNS\n",
            "  like: CS\n",
            "  an: AT\n",
            "  arrow: NN\n",
            "\n",
            "============================================================\n",
            "ANALYSIS AND COMPARISON\n",
            "============================================================\n",
            "\n",
            "1. Tagging differences for 'flies':\n",
            "   Rule-based: VBZ - could be noun(NNS) or verb(VBZ)\n",
            "   NLTK: NNS\n",
            "   Statistical: NNS\n",
            "\n",
            "2. Tagging differences for 'like':\n",
            "   Rule-based: IN - could be preposition(IN) or verb(VB)\n",
            "   NLTK: IN\n",
            "   Statistical: CS\n",
            "\n",
            "3. Two possible interpretations of the sentence:\n",
            "   a) Time passes quickly like an arrow (Time [flies like an arrow])\n",
            "      Structure: Noun + Verb phrase\n",
            "      Tagging: Time(NN) flies(VBZ) like(IN) an(DT) arrow(NN)\n",
            "      Meaning: Time passes as quickly as an arrow flies\n",
            "\n",
            "   b) Time-flies (insects) enjoy an arrow (Time flies [like an arrow])\n",
            "      Structure: Noun phrase + Verb + Noun phrase\n",
            "      Tagging: Time(NN) flies(NNS) like(VB) an(DT) arrow(NN)\n",
            "      Meaning: Insects called 'time flies' enjoy arrows\n",
            "\n",
            "4. Which method is best?\n",
            "   - Rule-based: Flexible but subjective\n",
            "   - NLTK: Based on extensive training data, usually most accurate\n",
            "   - Statistical: Based on corpus frequencies, needs sufficient data\n",
            "\n",
            "\n",
            "============================================================\n",
            "EXERCISE 2: Chinese Ambiguity - 'ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫'\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jieba is already installed\n",
            "Chinese sentence: ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫\n",
            "Pinyin: W«í x«êhuƒÅn chƒ´ p√≠nggu«í de r√©n\n",
            "Literal translation: I like eat apple DE person\n",
            "\n",
            "Method 1: jieba Word Segmentation\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading model cost 1.472 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.472 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation results: ['Êàë', 'ÂñúÊ¨¢', 'ÂêÉ', 'ËãπÊûú', 'ÁöÑ', '‰∫∫']\n",
            "POS tagging results:\n",
            "  Êàë: r\n",
            "  ÂñúÊ¨¢: v\n",
            "  ÂêÉ: v\n",
            "  ËãπÊûú: n\n",
            "  ÁöÑ: uj\n",
            "  ‰∫∫: n\n",
            "\n",
            "Tag meanings:\n",
            "  r - pronoun\n",
            "  v - verb\n",
            "  n - noun\n",
            "  uj - auxiliary particle\n",
            "\n",
            "Method 2: Rule-based Segmentation and POS Tagging\n",
            "----------------------------------------\n",
            "Three possible segmentation methods:\n",
            "  Method 1: Êàë | ÂñúÊ¨¢ | ÂêÉËãπÊûú | ÁöÑ | ‰∫∫\n",
            "  Method 2: ÊàëÂñúÊ¨¢ | ÂêÉ | ËãπÊûú | ÁöÑ | ‰∫∫\n",
            "  Method 3: Êàë | ÂñúÊ¨¢ | ÂêÉËãπÊûúÁöÑ‰∫∫\n",
            "\n",
            "Rule-based POS tagging (using segmentation method 1):\n",
            "  Êàë: PRON\n",
            "  ÂñúÊ¨¢: VERB\n",
            "  ÂêÉËãπÊûú: VP\n",
            "  ÁöÑ: PART\n",
            "  ‰∫∫: NOUN\n",
            "\n",
            "============================================================\n",
            "AMBIGUITY ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Two main interpretations:\n",
            "\n",
            "1. Interpretation A: I like [people who eat apples]\n",
            "   Structure: I + like + [people who eat apples]\n",
            "   Meaning: I like people who eat apples\n",
            "   Grammatical analysis:\n",
            "     - 'ÂêÉËãπÊûúÁöÑ' modifies '‰∫∫' (forms noun phrase)\n",
            "     - 'ÁöÑ' functions as a structural particle\n",
            "     - 'ÂêÉËãπÊûúÁöÑ‰∫∫' is the object of 'ÂñúÊ¨¢'\n",
            "\n",
            "2. Interpretation B: I like to eat apples\n",
            "   Structure: I + like + eat + apples\n",
            "   Meaning: I like eating apples ('ÁöÑ‰∫∫' might be redundant)\n",
            "   Grammatical analysis:\n",
            "     - 'ÂêÉËãπÊûú' is the object of 'ÂñúÊ¨¢'\n",
            "     - 'ÁöÑ' might indicate emphasis or colloquial expression\n",
            "\n",
            "3. Interpretation C: [I like eating apples]'s person (less common)\n",
            "   Structure: [I like eating apples] + ÁöÑ + person\n",
            "   Meaning: The person who likes eating apples\n",
            "   Grammatical analysis:\n",
            "     - 'ÊàëÂñúÊ¨¢ÂêÉËãπÊûú' modifies '‰∫∫' as a relative clause\n",
            "\n",
            "Key ambiguity points:\n",
            "   1. Grammatical role of 'ÁöÑ':\n",
            "      - Structural particle: connects modifier and head noun\n",
            "      - Might indicate possession or modification\n",
            "      - Could be a modal particle in some dialects\n",
            "\n",
            "   2. Syntactic function of 'ÂêÉËãπÊûú':\n",
            "      - As modifier: modifies '‰∫∫'\n",
            "      - As object: direct object of 'ÂñúÊ¨¢'\n",
            "\n",
            "   3. Structural hierarchy:\n",
            "      - (I like (people who eat apples))\n",
            "      - ((I like eating apples)'s person)\n",
            "\n",
            "Disambiguation strategies:\n",
            "   1. Context: surrounding sentences\n",
            "   2. Intonation/stress: in spoken language\n",
            "   3. Syntactic parsing: full parse tree analysis\n",
            "   4. Semantic analysis: agent and patient roles of 'ÂñúÊ¨¢'\n",
            "   5. Pragmatic factors: discourse context and speaker intent\n",
            "\n",
            "============================================================\n",
            "SUMMARY AND COMPARISON\n",
            "============================================================\n",
            "\n",
            "English sentence 'Time flies like an arrow':\n",
            "  - Ambiguity type: Structural + Lexical\n",
            "  - Key points: 'flies'(verb/noun), 'like'(preposition/verb)\n",
            "  - Method performance:\n",
            "     Rule-based: Flexible but requires manual rules\n",
            "     NLTK: Usually most accurate, based on large training data\n",
            "     Statistical: Needs sufficient corpus data\n",
            "\n",
            "Chinese sentence 'ÊàëÂñúÊ¨¢ÂêÉËãπÊûúÁöÑ‰∫∫':\n",
            "  - Ambiguity type: Structural + Elliptical\n",
            "  - Key points: Function of 'ÁöÑ', structural hierarchy\n",
            "  - Method performance:\n",
            "     jieba: Automatic segmentation but limited POS tagging\n",
            "     Rule-based: Requires linguistic knowledge\n",
            "\n",
            "General conclusions:\n",
            "  1. POS tagging is fundamental but insufficient for full disambiguation\n",
            "  2. Structural ambiguity requires syntactic analysis\n",
            "  3. Statistical methods generally outperform pure rule-based methods\n",
            "  4. Chinese processing is more complex than English\n",
            "  5. Context is crucial for disambiguation\n",
            "\n",
            "============================================================\n",
            "ADDITIONAL TEST SENTENCES\n",
            "============================================================\n",
            "\n",
            "1. Test sentence: We can can the can\n",
            "   Tokens: ['We', 'can', 'can', 'the', 'can']\n",
            "   POS tags: [('We', 'PRP'), ('can', 'MD'), ('can', 'MD'), ('the', 'DT'), ('can', 'MD')]\n",
            "   Ambiguity: 'can' can be modal verb(MD), verb(VB), or noun(NN)\n",
            "\n",
            "2. Test sentence: Fruit flies like a banana\n",
            "   Tokens: ['Fruit', 'flies', 'like', 'a', 'banana']\n",
            "   POS tags: [('Fruit', 'NNP'), ('flies', 'VBZ'), ('like', 'IN'), ('a', 'DT'), ('banana', 'NN')]\n",
            "   Ambiguity: 'flies' can be noun(fruit flies) or verb(flies)\n",
            "\n",
            "3. Test sentence: The old man the boat\n",
            "   Tokens: ['The', 'old', 'man', 'the', 'boat']\n",
            "   POS tags: [('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]\n",
            "   Ambiguity: 'man' can be noun(person) or verb(to operate)\n",
            "\n",
            "============================================================\n",
            "CODE EXECUTION COMPLETE!\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "TUTORIAL EXAMPLE: 'We can can the can'\n",
            "============================================================\n",
            "Sentence: We can can the can\n",
            "Tokens: ['We', 'can', 'can', 'the', 'can']\n",
            "POS tags: [('We', 'PRP'), ('can', 'MD'), ('can', 'MD'), ('the', 'DT'), ('can', 'MD')]\n",
            "\n",
            "Explanation of 'can' occurrences:\n",
            "  1. 'can' (position 1): Modal verb (MD) - indicates ability\n",
            "  2. 'can' (position 2): Verb (VB) - means to preserve/containerize\n",
            "  3. 'can' (position 4): Noun (NN) - refers to a container\n",
            "\n",
            "This demonstrates lexical ambiguity:\n",
            "  - Same word form 'can' has three different POS tags\n",
            "  - Context determines the correct interpretation\n",
            "  - Statistical models learn these patterns from training data\n"
          ]
        }
      ]
    }
  ]
}