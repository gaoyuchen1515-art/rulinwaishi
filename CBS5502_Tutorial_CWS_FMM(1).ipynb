{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaoyuchen1515-art/rulinwaishi/blob/main/CBS5502_Tutorial_CWS_FMM(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sve2XAV8pT5"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# ğŸ§  **CBS5502 â€” Computational Linguistics and NLP Technologies**\n",
        "\n",
        "### ğŸ **1st Python Tutorial**\n",
        "### ğŸ“… *January 28, 2026*\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ‡¨ğŸ‡³ **Chinese Word Segmentation**\n",
        "### âœ‚ï¸ *Forward Maximum Matching (FMM) Algorithm*\n",
        "### âœï¸ Apply to *Backward Maximum Matching (BMM) Algorithm*\n",
        "\n",
        "---\n",
        "### ğŸ‘¨â€ğŸ« **Instructor**\n",
        "**Dr. WAN Mingyu**\n",
        "\n",
        "### ğŸ‘¨â€ğŸ« **Teaching Assistant**\n",
        "**Mr. BAO Xiaoyi**\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸŒŸ Welcome!\n",
        "\n",
        "Welcome to the **first tutorial of CBS5502**!  \n",
        "In this session, we will step into the foundations of **Chinese Word Segmentation**, an important initial task in **Natural Language Processing (NLP)**, by implementing **self-built algorithms: FMM/BMM**.\n",
        "\n",
        "### ğŸ¯ Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "- âœ… Understand the task of **Chinese word segmentation**\n",
        "- âœ… Explain the **core idea of the FMM/BMM algorithms**\n",
        "- âœ… Implement **FMM and BMM from scratch in Python**\n",
        "- âœ… Compare the two methods with existing segmentation tools\n",
        "\n",
        "---\n",
        "This tutorial emphasizes **hands-on implementation** and **clear algorithmic thinking**.  No prior experience with Chinese NLP is requiredâ€”just curiosity and enthusiasm to learn!\n",
        "\n",
        "> ğŸ’¡ *Tip:* Read the explanations carefully and try modifying the code to deepen your understanding.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš€ Letâ€™s Get Started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ§  Step-by-Step Tutorial  \n",
        "## Chinese Word Segmentation with **Forward Maximum Matching (FMM)**  \n",
        "*(For Beginners with Little or No Python Experience)*\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ‡¨ğŸ‡³ Why Do We Need Chinese Word Segmentation?\n",
        "\n",
        "Unlike English, **Chinese sentences do not have spaces** between words.\n",
        "\n",
        "Example:\n",
        "\n",
        "```text\n",
        "ä»–å€‘é‚€è«‹äº†é›¢å³¶å€è­°å“¡åˆ—å¸­ã€‚\n",
        "\n",
        "Computers see this as one long string, but humans see words:\n",
        "\n",
        "ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶ / å€è­°å“¡ / åˆ—å¸­ / ã€‚ or\n",
        "ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶å€ / è­°å“¡ / åˆ—å¸­ /\n",
        "\n",
        "ğŸ‘‰ Word segmentation is the task of splitting a sentence into meaningful words.\n",
        "\n"
      ],
      "metadata": {
        "id": "73gi11-n8-W_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ‚ï¸ What Is Forward Maximum Matching (FMM)?\n",
        "\n",
        "**Forward Maximum Matching (FMM)** is a **simple and classical algorithm** widely used in  **Chinese word segmentation**.\n",
        "\n",
        "âœ¨ FMM is easy to understand, fast to implement, and a great starting point for learning Natural Language Processing (NLP).\n",
        "\n",
        "Because Chinese text does not contain spaces, FMM helps computers decide **where one word ends and the next begins**.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” How Does FMM Work?\n",
        "\n",
        "FMM follows these steps **from left to right**:\n",
        "\n",
        "1. **Start from the beginning** of the sentence  \n",
        "2. **Look ahead** and try to match the **longest possible word** in the dictionary  \n",
        "3. âœ… **If a word is found** â†’ accept it and move the pointer forward  \n",
        "4. âŒ **If no word is found** â†’ take **one single character** as a word  \n",
        "5. ğŸ” **Repeat** the process until the sentence ends\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Œ Key Idea\n",
        "\n",
        "> **â€œAlways try the longest word first.â€**\n",
        "\n",
        "This strategy helps reduce incorrect splitting by **prioritizing meaningful longer words** over shorter ones.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Simple Example\n",
        "\n",
        "Sentence:\n",
        "```text\n",
        "å—äº¬å¸‚é•·æ±Ÿå¤§æ©‹\n",
        "\n",
        "Dictionary contains: å—äº¬å¸‚, å—äº¬, å¸‚é•·, é•·æ±Ÿ, å¤§æ©‹, é•·æ±Ÿå¤§æ©‹\n",
        "\n",
        "FMM result:\n",
        "å—äº¬å¸‚ / é•·æ±Ÿå¤§æ©‹\n",
        "\n",
        "âœ… The algorithm prefers é•·æ±Ÿå¤§æ©‹ (4 characters)\n",
        "âŒ instead of splitting it into é•·æ±Ÿ / å¤§æ©‹\n",
        "\n"
      ],
      "metadata": {
        "id": "906ESzJH918i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ§  Implementation  \n",
        "## Forward Maximum Matching (FMM) for Chinese Word Segmentation\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "VoI-xi6V-9HU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQPF-XGn8pT8"
      },
      "source": [
        "## **Forward Maximum Matching algorithm for Chinese word segmentation**\n",
        "\n",
        "Replicate and understand the following codes for the task, and try a few things:\n",
        "\n",
        "- Change the input sentence with other interesting test samples\n",
        "- Adapt the current codes to BMM\n",
        "- Discuss the limitation of the current methods for word ambiguity\n",
        "- Extended Learning: call tokenizer in Jieba and compare the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rYHkotH8pT8",
        "outputId": "8d547693-be49-45f6-d6ac-300ec4cd1c46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Sentence: ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶ / å€è­°å“¡ / åˆ—å¸­ / ã€‚\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# Forward Maximum Matching (FMM)\n",
        "# Chinese Word Segmentation â€“ Beginner Friendly Version\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def forward_maximum_matching(sentence, dictionary):\n",
        "    \"\"\"\n",
        "    Segment a Chinese sentence using Forward Maximum Matching (FMM).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentence : str\n",
        "        The Chinese sentence to be segmented.\n",
        "    dictionary : set\n",
        "        A set of known words.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of segmented words.\n",
        "    \"\"\"\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Step 1: Find the maximum word length in the dictionary\n",
        "    # --------------------------------------------------------\n",
        "    # This tells us how many characters we should try at most.\n",
        "    max_word_length = 0\n",
        "    for word in dictionary:\n",
        "        if len(word) > max_word_length:\n",
        "            max_word_length = len(word)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Step 2: Prepare variables\n",
        "    # --------------------------------------------------------\n",
        "    segmented_words = []   # This list will store the result\n",
        "    current_position = 0   # Pointer to the current position in the sentence\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Step 3: Process the sentence from left to right\n",
        "    # --------------------------------------------------------\n",
        "    while current_position < len(sentence):\n",
        "\n",
        "        # Assume no word is matched at the current position\n",
        "        found_word = False\n",
        "\n",
        "        # ----------------------------------------------------\n",
        "        # Step 4: Try to match the longest word first\n",
        "        # ----------------------------------------------------\n",
        "        for word_length in range(max_word_length, 0, -1):\n",
        "\n",
        "            # If the word goes beyond the sentence, skip it\n",
        "            if current_position + word_length > len(sentence):\n",
        "                continue\n",
        "\n",
        "            # Extract a candidate word from the sentence\n",
        "            candidate_word = sentence[\n",
        "                current_position : current_position + word_length\n",
        "            ]\n",
        "\n",
        "            # Check if this word exists in the dictionary\n",
        "            if candidate_word in dictionary:\n",
        "                segmented_words.append(candidate_word)\n",
        "                current_position += word_length\n",
        "                found_word = True\n",
        "                break  # Stop after finding the longest match\n",
        "\n",
        "        # ----------------------------------------------------\n",
        "        # Step 5: If no word is found, take one character\n",
        "        # ----------------------------------------------------\n",
        "        if not found_word:\n",
        "            segmented_words.append(sentence[current_position])\n",
        "            current_position += 1\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Step 6: Return the segmentation result\n",
        "    # --------------------------------------------------------\n",
        "    return segmented_words\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Example Usage\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Dictionary (known words)\n",
        "dictionary = {\n",
        "    \"ä»–å€‘\", \"æˆ‘å€‘\", \"é‚€è«‹\", \"å—é‚€\", \"äº†\", \"æ˜¯\",\n",
        "    \"é›¢å³¶\", \"æ¸¯å³¶\", \"å€è­°å“¡\", \"åˆ—å¸­\", \"å‡ºå¸­\", \"å€\", \"è­°å“¡\"\n",
        "}\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"ä»–å€‘é‚€è«‹äº†é›¢å³¶å€è­°å“¡åˆ—å¸­ã€‚\"\n",
        "\n",
        "# Run FMM segmentation\n",
        "result = forward_maximum_matching(sentence, dictionary)\n",
        "\n",
        "# Display the result\n",
        "print(\"Segmented Sentence:\")\n",
        "print(\" / \".join(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## âœï¸ Exercise 1: Backward Maximum Matching (BMM)\n",
        "\n",
        "In this exercise, you will **modify the Forward Maximum Matching (FMM) code**  \n",
        "to implement **Backward Maximum Matching (BMM)** for **Chinese Word Segmentation**.\n",
        "\n",
        "This task helps you understand **how direction affects segmentation**.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”„ What Is Backward Maximum Matching (BMM)?\n",
        "\n",
        "**Backward Maximum Matching (BMM)** works almost the same as FMM, **but in reverse**:\n",
        "\n",
        "| FMM | BMM |\n",
        "|----|----|\n",
        "| Start from the **beginning** | Start from the **end** |\n",
        "| Move **left â†’ right** | Move **right â†’ left** |\n",
        "| Try longest word first | Try longest word first |\n",
        "\n",
        "ğŸ“Œ **Key idea**:  \n",
        "> *Always try the longest word, but from the end of the sentence.*\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Your Task\n",
        "\n",
        "You will:\n",
        "\n",
        "1. **Reuse the FMM code**\n",
        "2. Change the logic to:\n",
        "   - Start from the **end** of the sentence\n",
        "   - Move **backward**\n",
        "3. Segment the sentence using **BMM**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LqOQyDguCNHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Backward Maximum Matching (BMM)\n",
        "# Chinese Word Segmentation â€“ Beginner Friendly Version\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def backward_maximum_matching(sentence, dictionary):\n",
        "    \"\"\"\n",
        "    Segment a Chinese sentence using Backward Maximum Matching (BMM).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentence : str\n",
        "        The Chinese sentence to be segmented.\n",
        "    dictionary : set\n",
        "        A set of known words.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        A list of segmented words.\n",
        "    \"\"\"\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Step 1: Find the maximum word length in the dictionary\n",
        "    # --------------------------------------------------------\n",
        "    # This tells us how many characters we should try at most.\n",
        "    max_word_length = 0\n",
        "    for word in dictionary:\n",
        "        if len(word) > max_word_length:\n",
        "            max_word_length = len(word)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Step 2: Prepare variables\n",
        "    # --------------------------------------------------------\n",
        "    segmented_words = []   # This list will store the result\n",
        "    current_position = len(sentence)   # Pointer to the current position in the sentence (start from end)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Step 3: Process the sentence from right to left\n",
        "    # --------------------------------------------------------\n",
        "    while current_position > 0:\n",
        "\n",
        "        # Assume no word is matched at the current position\n",
        "        found_word = False\n",
        "\n",
        "        # ----------------------------------------------------\n",
        "        # Step 4: Try to match the longest word first\n",
        "        # ----------------------------------------------------\n",
        "        for word_length in range(max_word_length, 0, -1):\n",
        "\n",
        "            # If the word goes beyond the sentence beginning, skip it\n",
        "            if current_position - word_length < 0:\n",
        "                continue\n",
        "\n",
        "            # Extract a candidate word from the sentence (from left to right indexing)\n",
        "            candidate_word = sentence[\n",
        "                current_position - word_length : current_position\n",
        "            ]\n",
        "\n",
        "            # Check if this word exists in the dictionary\n",
        "            if candidate_word in dictionary:\n",
        "                segmented_words.append(candidate_word)\n",
        "                current_position -= word_length\n",
        "                found_word = True\n",
        "                break  # Stop after finding the longest match\n",
        "\n",
        "        # ----------------------------------------------------\n",
        "        # Step 5: If no word is found, take one character\n",
        "        # ----------------------------------------------------\n",
        "        if not found_word:\n",
        "            # Take one character from the left of current position\n",
        "            segmented_words.append(sentence[current_position - 1])\n",
        "            current_position -= 1\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Step 6: Reverse the result (since we built it backwards)\n",
        "    # --------------------------------------------------------\n",
        "    segmented_words.reverse()\n",
        "\n",
        "    return segmented_words\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Example Usage\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Dictionary (known words)\n",
        "dictionary = {\n",
        "    \"ä»–å€‘\", \"æˆ‘å€‘\", \"é‚€è«‹\", \"å—é‚€\", \"äº†\", \"æ˜¯\",\n",
        "    \"é›¢å³¶\", \"æ¸¯å³¶\", \"å€è­°å“¡\", \"åˆ—å¸­\", \"å‡ºå¸­\", \"å€\", \"è­°å“¡\"\n",
        "}\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"ä»–å€‘é‚€è«‹äº†é›¢å³¶å€è­°å“¡åˆ—å¸­ã€‚\"\n",
        "\n",
        "# Run BMM segmentation\n",
        "result = backward_maximum_matching(sentence, dictionary)\n",
        "\n",
        "# Display the result\n",
        "print(\"Backward Maximum Matching (BMM) Result:\")\n",
        "print(\" / \".join(result))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Optional: Compare with FMM result\n",
        "# ------------------------------------------------------------\n",
        "def forward_maximum_matching(sentence, dictionary):\n",
        "    \"\"\"FMM for comparison\"\"\"\n",
        "    max_word_length = 0\n",
        "    for word in dictionary:\n",
        "        if len(word) > max_word_length:\n",
        "            max_word_length = len(word)\n",
        "\n",
        "    segmented_words = []\n",
        "    current_position = 0\n",
        "\n",
        "    while current_position < len(sentence):\n",
        "        found_word = False\n",
        "        for word_length in range(max_word_length, 0, -1):\n",
        "            if current_position + word_length > len(sentence):\n",
        "                continue\n",
        "            candidate_word = sentence[current_position : current_position + word_length]\n",
        "            if candidate_word in dictionary:\n",
        "                segmented_words.append(candidate_word)\n",
        "                current_position += word_length\n",
        "                found_word = True\n",
        "                break\n",
        "        if not found_word:\n",
        "            segmented_words.append(sentence[current_position])\n",
        "            current_position += 1\n",
        "    return segmented_words\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Comparison between BMM and FMM:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "fmm_result = forward_maximum_matching(sentence, dictionary)\n",
        "print(f\"FMM Result: {' / '.join(fmm_result)}\")\n",
        "print(f\"BMM Result: {' / '.join(result)}\")\n",
        "\n",
        "# Check if results are the same\n",
        "if fmm_result == result:\n",
        "    print(\"\\nâœ… Both algorithms give the same result!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Results are different!\")\n",
        "    print(f\"Difference in segmentation:\")\n",
        "    print(f\"  FMM segments: {len(fmm_result)}\")\n",
        "    print(f\"  BMM segments: {len(result)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slfob8cJvrMw",
        "outputId": "c273e853-b5e2-455b-f442-9598741b5605"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backward Maximum Matching (BMM) Result:\n",
            "ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶ / å€è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "\n",
            "==================================================\n",
            "Comparison between BMM and FMM:\n",
            "==================================================\n",
            "FMM Result: ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶ / å€è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "BMM Result: ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶ / å€è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "\n",
            "âœ… Both algorithms give the same result!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## âœï¸ Exercise 2: Using Existing Word Segmentation Tools  \n",
        "### Comparing **FMM / BMM**, **NLTK**, and **Jieba**\n",
        "\n",
        "In this exercise, you will **apply existing word segmentation tools** to the **same Chinese sentence** and **compare the results** with your own **FMM/BMM implementations**.\n",
        "\n",
        "The goal is to understand:\n",
        "\n",
        "- âœ… How real NLP tools perform word segmentation\n",
        "- âœ… The **strengths and weaknesses** of rule-based vs. statistical methods\n",
        "- âœ… How **word meaning resolution (ambiguity)** is handled differently\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Task Overview\n",
        "\n",
        "You will:\n",
        "\n",
        "1. Segment the same sentence using:\n",
        "   - âœ… Your own **FMM / BMM**\n",
        "   - âœ… **NLTK** (baseline / limitation observation)\n",
        "   - âœ… **Jieba**\n",
        "2. Compare the segmentation results\n",
        "3. Discuss **why results differ**\n",
        "4. Analyze how each method handles **ambiguity and word meaning**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§ª Test Sentence (Same as Before)\n",
        "\n",
        "```python\n",
        "sentence = \"ä»–å€‘é‚€è«‹äº†é›¢å³¶å€è­°å“¡åˆ—å¸­ã€‚\""
      ],
      "metadata": {
        "id": "sX8rNNknDDIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Prepare the sentence and dictionary\n",
        "# ----------------------------\n",
        "sentence = \"ä»–å€‘é‚€è«‹äº†é›¢å³¶å€è­°å“¡åˆ—å¸­ã€‚\"\n",
        "dictionary = {\n",
        "    \"ä»–å€‘\", \"æˆ‘å€‘\", \"é‚€è«‹\", \"å—é‚€\", \"äº†\", \"æ˜¯\",\n",
        "    \"é›¢å³¶\", \"æ¸¯å³¶\", \"å€è­°å“¡\", \"åˆ—å¸­\", \"å‡ºå¸­\", \"å€\", \"è­°å“¡\"\n",
        "}\n",
        "\n",
        "print(f\"Test Sentence: {sentence}\")\n",
        "print(f\"Dictionary Size: {len(dictionary)} words\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# 2. Forward Maximum Matching (FMM) - Self-built\n",
        "# ============================================================\n",
        "\n",
        "def forward_maximum_matching(sentence, dictionary):\n",
        "    \"\"\"Forward Maximum Matching Algorithm for Chinese Word Segmentation\"\"\"\n",
        "\n",
        "    # Step 1: Find maximum word length in dictionary\n",
        "    max_word_length = 0\n",
        "    for word in dictionary:\n",
        "        if len(word) > max_word_length:\n",
        "            max_word_length = len(word)\n",
        "\n",
        "    # Step 2: Initialize variables\n",
        "    segmented_words = []\n",
        "    current_position = 0\n",
        "\n",
        "    # Step 3: Process from left to right\n",
        "    while current_position < len(sentence):\n",
        "        found_word = False\n",
        "\n",
        "        # Step 4: Try to match longest word first\n",
        "        for word_length in range(max_word_length, 0, -1):\n",
        "            if current_position + word_length > len(sentence):\n",
        "                continue\n",
        "\n",
        "            candidate_word = sentence[current_position:current_position + word_length]\n",
        "\n",
        "            if candidate_word in dictionary:\n",
        "                segmented_words.append(candidate_word)\n",
        "                current_position += word_length\n",
        "                found_word = True\n",
        "                break\n",
        "\n",
        "        # Step 5: If no word found, take single character\n",
        "        if not found_word:\n",
        "            segmented_words.append(sentence[current_position])\n",
        "            current_position += 1\n",
        "\n",
        "    # Step 6: Return result\n",
        "    return segmented_words\n",
        "\n",
        "print(\"\\n=== Forward Maximum Matching (FMM) Result ===\")\n",
        "fmm_result = forward_maximum_matching(sentence, dictionary)\n",
        "print(\"Segmentation: \" + \" / \".join(fmm_result))\n",
        "print(f\"Number of segments: {len(fmm_result)}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. Backward Maximum Matching (BMM) - Self-built\n",
        "# ============================================================\n",
        "\n",
        "def backward_maximum_matching(sentence, dictionary):\n",
        "    \"\"\"Backward Maximum Matching Algorithm for Chinese Word Segmentation\"\"\"\n",
        "\n",
        "    # Step 1: Find maximum word length in dictionary\n",
        "    max_word_length = 0\n",
        "    for word in dictionary:\n",
        "        if len(word) > max_word_length:\n",
        "            max_word_length = len(word)\n",
        "\n",
        "    # Step 2: Initialize variables (start from end)\n",
        "    segmented_words = []\n",
        "    current_position = len(sentence)\n",
        "\n",
        "    # Step 3: Process from right to left\n",
        "    while current_position > 0:\n",
        "        found_word = False\n",
        "\n",
        "        # Step 4: Try to match longest word first\n",
        "        for word_length in range(max_word_length, 0, -1):\n",
        "            if current_position - word_length < 0:\n",
        "                continue\n",
        "\n",
        "            candidate_word = sentence[current_position - word_length:current_position]\n",
        "\n",
        "            if candidate_word in dictionary:\n",
        "                segmented_words.append(candidate_word)\n",
        "                current_position -= word_length\n",
        "                found_word = True\n",
        "                break\n",
        "\n",
        "        # Step 5: If no word found, take single character\n",
        "        if not found_word:\n",
        "            segmented_words.append(sentence[current_position - 1])\n",
        "            current_position -= 1\n",
        "\n",
        "    # Step 6: Reverse the result (since we built it backwards)\n",
        "    segmented_words.reverse()\n",
        "    return segmented_words\n",
        "\n",
        "print(\"\\n=== Backward Maximum Matching (BMM) Result ===\")\n",
        "bmm_result = backward_maximum_matching(sentence, dictionary)\n",
        "print(\"Segmentation: \" + \" / \".join(bmm_result))\n",
        "print(f\"Number of segments: {len(bmm_result)}\")\n",
        "\n",
        "# Compare FMM and BMM\n",
        "print(\"\\n--- FMM vs BMM Comparison ---\")\n",
        "if fmm_result == bmm_result:\n",
        "    print(\"âœ… FMM and BMM results are identical\")\n",
        "else:\n",
        "    print(\"âš ï¸  FMM and BMM results are different\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. NLTK (Natural Language Toolkit) - FIXED VERSION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"=== NLTK Tokenization ===\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install and import NLTK\n",
        "!pip install nltk -q\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Initialize nltk_result variable with default value\n",
        "nltk_result = []\n",
        "\n",
        "try:\n",
        "    # Download necessary NLTK data with more comprehensive setup\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "    nltk_result = nltk.word_tokenize(sentence)\n",
        "    print(\"Segmentation: \" + \" / \".join(nltk_result))\n",
        "    print(f\"Number of segments: {len(nltk_result)}\")\n",
        "    print(\"\\nâš ï¸  Note: NLTK is primarily designed for English.\")\n",
        "    print(\"   It tokenizes Chinese text character by character.\")\n",
        "except Exception as e:\n",
        "    # Fallback: if NLTK fails, use character splitting\n",
        "    print(\"NLTK tokenizer failed, using fallback character splitting.\")\n",
        "    nltk_result = [char for char in sentence if char.strip()]\n",
        "    print(\"Segmentation: \" + \" / \".join(nltk_result))\n",
        "    print(f\"Number of segments: {len(nltk_result)}\")\n",
        "    print(f\"\\nNLTK Error: {str(e)[:100]}...\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. Jieba (Chinese Text Segmentation)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"=== Jieba Segmentation ===\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install and import Jieba\n",
        "!pip install jieba -q\n",
        "\n",
        "import jieba\n",
        "\n",
        "# Exact Mode (Default)\n",
        "print(\"\\n--- Jieba Exact Mode (Default) ---\")\n",
        "jieba_result = list(jieba.cut(sentence))\n",
        "print(\"Segmentation: \" + \" / \".join(jieba_result))\n",
        "print(f\"Number of segments: {len(jieba_result)}\")\n",
        "\n",
        "# Full Mode\n",
        "print(\"\\n--- Jieba Full Mode ---\")\n",
        "jieba_full_result = list(jieba.cut(sentence, cut_all=True))\n",
        "print(\"Segmentation: \" + \" / \".join(jieba_full_result))\n",
        "print(f\"Number of segments: {len(jieba_full_result)}\")\n",
        "\n",
        "# Search Engine Mode\n",
        "print(\"\\n--- Jieba Search Engine Mode ---\")\n",
        "jieba_search_result = list(jieba.cut_for_search(sentence))\n",
        "print(\"Segmentation: \" + \" / \".join(jieba_search_result))\n",
        "print(f\"Number of segments: {len(jieba_search_result)}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. Ambiguity Test (Additional Analysis)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"=== Ambiguity Analysis ===\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test with ambiguous sentence\n",
        "ambiguous_sentence = \"å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥\"\n",
        "ambiguous_dict = {\"å—äº¬å¸‚\", \"å—äº¬\", \"å¸‚é•¿\", \"é•¿æ±Ÿ\", \"å¤§æ¡¥\", \"é•¿æ±Ÿå¤§æ¡¥\", \"å¸‚\", \"æ±Ÿ\", \"å¤§\"}\n",
        "\n",
        "print(f\"Ambiguous Sentence: {ambiguous_sentence}\")\n",
        "print(f\"Dictionary: {ambiguous_dict}\")\n",
        "\n",
        "# FMM on ambiguous sentence\n",
        "fmm_amb = forward_maximum_matching(ambiguous_sentence, ambiguous_dict)\n",
        "print(f\"\\nFMM Result: {' / '.join(fmm_amb)}\")\n",
        "\n",
        "# BMM on ambiguous sentence\n",
        "bmm_amb = backward_maximum_matching(ambiguous_sentence, ambiguous_dict)\n",
        "print(f\"BMM Result: {' / '.join(bmm_amb)}\")\n",
        "\n",
        "# Jieba on ambiguous sentence\n",
        "jieba_amb = list(jieba.cut(ambiguous_sentence))\n",
        "print(f\"Jieba Result: {' / '.join(jieba_amb)}\")\n",
        "\n",
        "if fmm_amb != bmm_amb:\n",
        "    print(\"\\nâš ï¸  FMM and BMM give different results for ambiguous sentence!\")\n",
        "else:\n",
        "    print(\"\\nâœ… FMM and BMM give the same result for ambiguous sentence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BzmDVAbyGQx",
        "outputId": "7bc4d89c-4d9b-45b3-db42-07a420e3626f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Sentence: ä»–å€‘é‚€è«‹äº†é›¢å³¶å€è­°å“¡åˆ—å¸­ã€‚\n",
            "Dictionary Size: 13 words\n",
            "============================================================\n",
            "\n",
            "=== Forward Maximum Matching (FMM) Result ===\n",
            "Segmentation: ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶ / å€è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "Number of segments: 7\n",
            "\n",
            "=== Backward Maximum Matching (BMM) Result ===\n",
            "Segmentation: ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶ / å€è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "Number of segments: 7\n",
            "\n",
            "--- FMM vs BMM Comparison ---\n",
            "âœ… FMM and BMM results are identical\n",
            "\n",
            "============================================================\n",
            "=== NLTK Tokenization ===\n",
            "============================================================\n",
            "Segmentation: ä»–å€‘é‚€è«‹äº†é›¢å³¶å€è­°å“¡åˆ—å¸­ã€‚\n",
            "Number of segments: 1\n",
            "\n",
            "âš ï¸  Note: NLTK is primarily designed for English.\n",
            "   It tokenizes Chinese text character by character.\n",
            "\n",
            "============================================================\n",
            "=== Jieba Segmentation ===\n",
            "============================================================\n",
            "\n",
            "--- Jieba Exact Mode (Default) ---\n",
            "Segmentation: ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶å€ / è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "Number of segments: 7\n",
            "\n",
            "--- Jieba Full Mode ---\n",
            "Segmentation: ä»– / å€‘ / é‚€ / è«‹ / äº† / é›¢ / å³¶ / å€ / è­° / å“¡ / åˆ—å¸­ / ã€‚\n",
            "Number of segments: 12\n",
            "\n",
            "--- Jieba Search Engine Mode ---\n",
            "Segmentation: ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶å€ / è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "Number of segments: 7\n",
            "\n",
            "============================================================\n",
            "=== Ambiguity Analysis ===\n",
            "============================================================\n",
            "Ambiguous Sentence: å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥\n",
            "Dictionary: {'å¤§', 'å—äº¬', 'é•¿æ±Ÿ', 'æ±Ÿ', 'å¸‚é•¿', 'é•¿æ±Ÿå¤§æ¡¥', 'å—äº¬å¸‚', 'å¸‚', 'å¤§æ¡¥'}\n",
            "\n",
            "FMM Result: å—äº¬å¸‚ / é•¿æ±Ÿå¤§æ¡¥\n",
            "BMM Result: å—äº¬å¸‚ / é•¿æ±Ÿå¤§æ¡¥\n",
            "Jieba Result: å—äº¬å¸‚ / é•¿æ±Ÿå¤§æ¡¥\n",
            "\n",
            "âœ… FMM and BMM give the same result for ambiguous sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Exercise: Comparing Word Segmentation Methods\n",
        "# FMM / BMM (Self-built) vs NLTK vs Jieba\n",
        "# ============================================================\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Prepare the sentence\n",
        "# ----------------------------\n",
        "sentence = \"ä»–å€‘é‚€è«‹äº†é›¢å³¶å€è­°å“¡åˆ—å¸­ã€‚\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. NLTK (reference code Only)\n",
        "# ============================================================\n",
        "\n",
        "# Install and import NLTK\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "\n",
        "print(\"=== NLTK Tokenization Result ===\")\n",
        "try:\n",
        "    nltk_result = nltk.word_tokenize(sentence)\n",
        "    print(nltk_result)\n",
        "except Exception as e:\n",
        "    print(\"NLTK failed to tokenize Chinese properly.\")\n",
        "    print(\"Error:\", e)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. Jieba (reference code only)\n",
        "# ============================================================\n",
        "\n",
        "# Install and import Jieba\n",
        "!pip install jieba\n",
        "\n",
        "import jieba\n",
        "\n",
        "print(\"\\n=== Jieba Segmentation Result ===\")\n",
        "jieba_result = list(jieba.cut(sentence))\n",
        "print(\" / \".join(jieba_result))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. (Optional) Jieba Different Modes\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Jieba Full Mode ===\")\n",
        "print(\" / \".join(jieba.cut(sentence, cut_all=True)))\n",
        "\n",
        "print(\"\\n=== Jieba Search Mode ===\")\n",
        "print(\" / \".join(jieba.cut_for_search(sentence)))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. Discussion Reminder (for Students)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Discussion Points ===\")\n",
        "print(\"1. Why does NLTK not work well for Chinese?\")\n",
        "print(\"2. How does Jieba differ from FMM/BMM?\")\n",
        "print(\"3. Which method handles word meaning ambiguity better?\")\n",
        "print(\"4. Why are statistical models important in NLP?\")"
      ],
      "metadata": {
        "id": "Lj0raQq_DYbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89089e7a-ce7f-4a0d-ea33-80293c1ef06a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "=== NLTK Tokenization Result ===\n",
            "NLTK failed to tokenize Chinese properly.\n",
            "Error: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('punkt_tab')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n",
            "\n",
            "=== Jieba Segmentation Result ===\n",
            "ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶å€ / è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "\n",
            "=== Jieba Full Mode ===\n",
            "ä»– / å€‘ / é‚€ / è«‹ / äº† / é›¢ / å³¶ / å€ / è­° / å“¡ / åˆ—å¸­ / ã€‚\n",
            "\n",
            "=== Jieba Search Mode ===\n",
            "ä»–å€‘ / é‚€è«‹ / äº† / é›¢å³¶å€ / è­°å“¡ / åˆ—å¸­ / ã€‚\n",
            "\n",
            "=== Discussion Points ===\n",
            "1. Why does NLTK not work well for Chinese?\n",
            "2. How does Jieba differ from FMM/BMM?\n",
            "3. Which method handles word meaning ambiguity better?\n",
            "4. Why are statistical models important in NLP?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ğŸ‰ Tutorial Wrapâ€‘Up & Submission Reminder\n",
        "\n",
        "Congratulations on completing **Week 3â€™s tutorial** for **CBS5502**!  \n",
        "You have taken an important step into **Natural Language Processing (NLP)** by:\n",
        "\n",
        "- âœ… Understanding **Chinese Word Segmentation**\n",
        "- âœ… Implementing **Forward Maximum Matching (FMM)**\n",
        "- âœ… Extending it to **Backward Maximum Matching (BMM)**\n",
        "- âœ… Comparing ruleâ€‘based methods with real NLP tools (**NLTK & Jieba**)\n",
        "- âœ… Reflecting on **word meaning resolution and ambiguity**\n",
        "\n",
        "These skills form a **critical foundation** for more advanced NLP models and techniques.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Œ Participation Submission Reminder\n",
        "\n",
        "Please remember to submit your participation work, which consists of completing **Exercises 1 and 2** to practice this weekâ€™s learning.\n",
        "\n",
        "### ğŸ“‚ What to Submit\n",
        "- âœ… Your completed **Week 3 Participation `yourname.ipynb` notebook**\n",
        "\n",
        "### ğŸ“ Where to Submit\n",
        "- **Blackboard (BB)**\n",
        "- Folder: **â€œWeek 3 Participationâ€**\n",
        "\n",
        "### â° Deadline\n",
        "- **By the end of Week 3**\n",
        "- **ğŸ“… February 1, 2026**\n",
        "\n",
        "âš ï¸ Late submissions may affect your participation grade, so please plan accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ Final Notes\n",
        "\n",
        "- Make sure your notebook **runs without errors**\n",
        "- Include **code, outputs, and brief reflections**\n",
        "- Clearly show your work on **FMM, BMM, and tool comparisons**\n",
        "\n",
        "If you have any questions, feel free to ask during tutorials or on the course forum.\n",
        "\n",
        "---\n",
        "\n",
        "âœ¨ **Well done, and keep up the great work!**  \n",
        "We look forward to exploring more exciting NLP topics with you in the coming weeks ğŸš€"
      ],
      "metadata": {
        "id": "732ZDVj5EPxv"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}